{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.contrib import learn\n",
    "from itertools import repeat, chain\n",
    "\n",
    "from kim_cnn import KimCNN\n",
    "from eval_helpers import precision_at_ks, label_lists_to_sparse_tuple\n",
    "from data_helpers import batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DATA_DIR=data/stackexchange/datascience/\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "MAX_DOCUMENT_LENGTH=2000\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "TAG_FREQ_THRESHOLD=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.flags.DEFINE_string('data_dir', 'data/stackexchange/datascience/', 'directory of dataset')\n",
    "tf.flags.DEFINE_integer('tag_freq_threshold', 5, 'minimum frequency of a tag')\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_float(\"max_document_length\", 2000, \"Maximum length of document, exceeding part is truncated\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "data_dir = FLAGS.data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join(data_dir, \"input_text.csv\")\n",
    "tdf = pd.read_csv(text_path, header=None)\n",
    "x_text = tdf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(FLAGS.max_document_length)\n",
    "X = np.array(list(vocab_processor.fit_transform(x_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load train/test data\n",
    "Y_labels = pkl.load(open(os.path.join(data_dir, \"Y.pkl\"), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = sum(len(ls) for ls in Y_labels)\n",
    "row_indx = list(chain(*[list(repeat(i, len(ls))) for i, ls in enumerate(Y_labels)]))\n",
    "col_indx = list(chain(*Y_labels))\n",
    "Y_binary = csr_matrix((np.ones(size), (row_indx, col_indx)), shape=(len(Y_labels), len(set(col_indx)))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 5364/596\n",
      "num of classes: 119\n"
     ]
    }
   ],
   "source": [
    "# convert dtype to be compatible with fastxml\n",
    "# X.data = np.asarray(X.data, dtype=np.float32)\n",
    "\n",
    "# split data\n",
    "x_train, x_dev, y_train_binary, y_dev_binary, y_train_labels, y_dev_labels = train_test_split(\n",
    "    X, Y_binary, Y_labels, train_size=1 - FLAGS.dev_sample_percentage, random_state=42)\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev)))\n",
    "\n",
    "num_classes = y_train_binary.shape[1]\n",
    "print(\"num of classes: {:d}\".format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use softmax xentropy\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /home/cloud-user/code/network_embedding/runs/1504724346\n",
      "\n",
      "2017-09-06T18:59:08.823077: step 1, loss 34.2788, p1 0.015625, p3 0.0104167, p5 0.01875\n",
      "2017-09-06T18:59:10.516814: step 2, loss 31.3422, p1 0.015625, p3 0.015625, p5 0.015625\n",
      "2017-09-06T18:59:12.238820: step 3, loss 28.3899, p1 0.046875, p3 0.015625, p5 0.01875\n",
      "2017-09-06T18:59:13.904694: step 4, loss 24.8319, p1 0.0625, p3 0.0520833, p5 0.04375\n",
      "2017-09-06T18:59:15.554344: step 5, loss 23.3464, p1 0.0625, p3 0.0416667, p5 0.0375\n",
      "2017-09-06T18:59:17.204918: step 6, loss 23.0512, p1 0.0625, p3 0.046875, p5 0.046875\n",
      "2017-09-06T18:59:18.897090: step 7, loss 23.569, p1 0.078125, p3 0.0520833, p5 0.053125\n",
      "2017-09-06T18:59:20.548445: step 8, loss 21.3848, p1 0.109375, p3 0.046875, p5 0.040625\n",
      "2017-09-06T18:59:22.215005: step 9, loss 20.1061, p1 0.078125, p3 0.0520833, p5 0.040625\n",
      "2017-09-06T18:59:23.879104: step 10, loss 18.6518, p1 0.046875, p3 0.0572917, p5 0.05\n",
      "2017-09-06T18:59:25.571835: step 11, loss 19.8492, p1 0.078125, p3 0.0833333, p5 0.06875\n",
      "2017-09-06T18:59:27.258828: step 12, loss 18.0875, p1 0.0625, p3 0.046875, p5 0.05\n",
      "2017-09-06T18:59:29.003737: step 13, loss 17.7413, p1 0.015625, p3 0.0416667, p5 0.04375\n",
      "2017-09-06T18:59:30.717787: step 14, loss 18.0164, p1 0.0625, p3 0.0625, p5 0.05\n",
      "2017-09-06T18:59:32.432682: step 15, loss 15.8964, p1 0.078125, p3 0.0677083, p5 0.059375\n",
      "2017-09-06T18:59:34.107830: step 16, loss 15.9833, p1 0.078125, p3 0.078125, p5 0.05625\n",
      "2017-09-06T18:59:35.822640: step 17, loss 16.7809, p1 0.0625, p3 0.0677083, p5 0.0625\n",
      "2017-09-06T18:59:37.526228: step 18, loss 16.4273, p1 0.046875, p3 0.03125, p5 0.034375\n",
      "2017-09-06T18:59:39.210264: step 19, loss 18.22, p1 0.078125, p3 0.0833333, p5 0.071875\n",
      "2017-09-06T18:59:40.898386: step 20, loss 16.8782, p1 0.109375, p3 0.09375, p5 0.065625\n",
      "2017-09-06T18:59:42.574064: step 21, loss 15.9194, p1 0.109375, p3 0.0885417, p5 0.071875\n",
      "2017-09-06T18:59:44.280291: step 22, loss 15.1835, p1 0.125, p3 0.0885417, p5 0.084375\n",
      "2017-09-06T18:59:45.982077: step 23, loss 16.2672, p1 0.109375, p3 0.0989583, p5 0.08125\n",
      "2017-09-06T18:59:47.678296: step 24, loss 16.7712, p1 0.171875, p3 0.0885417, p5 0.078125\n",
      "2017-09-06T18:59:49.372216: step 25, loss 15.7419, p1 0.125, p3 0.09375, p5 0.059375\n",
      "2017-09-06T18:59:51.065657: step 26, loss 14.6765, p1 0.109375, p3 0.0833333, p5 0.0875\n",
      "2017-09-06T18:59:52.753466: step 27, loss 15.4235, p1 0.09375, p3 0.0885417, p5 0.075\n",
      "2017-09-06T18:59:54.463332: step 28, loss 13.6071, p1 0.078125, p3 0.0625, p5 0.046875\n",
      "2017-09-06T18:59:56.136713: step 29, loss 15.2122, p1 0.078125, p3 0.0885417, p5 0.075\n",
      "2017-09-06T18:59:57.835795: step 30, loss 14.7537, p1 0.09375, p3 0.0885417, p5 0.059375\n",
      "2017-09-06T18:59:59.513821: step 31, loss 14.6978, p1 0.125, p3 0.09375, p5 0.071875\n",
      "2017-09-06T19:00:01.193007: step 32, loss 15.459, p1 0.109375, p3 0.078125, p5 0.059375\n",
      "2017-09-06T19:00:02.864178: step 33, loss 14.0115, p1 0.0625, p3 0.0729167, p5 0.084375\n",
      "2017-09-06T19:00:04.535054: step 34, loss 14.5131, p1 0.109375, p3 0.0625, p5 0.065625\n",
      "2017-09-06T19:00:06.222865: step 35, loss 14.4674, p1 0.078125, p3 0.0677083, p5 0.065625\n",
      "2017-09-06T19:00:07.901403: step 36, loss 14.1105, p1 0.125, p3 0.0729167, p5 0.071875\n",
      "2017-09-06T19:00:09.568584: step 37, loss 14.702, p1 0.09375, p3 0.0625, p5 0.0625\n",
      "2017-09-06T19:00:11.225994: step 38, loss 15.5917, p1 0.0625, p3 0.0677083, p5 0.05\n",
      "2017-09-06T19:00:12.922552: step 39, loss 15.2678, p1 0.0625, p3 0.0677083, p5 0.053125\n",
      "2017-09-06T19:00:14.619949: step 40, loss 15.5859, p1 0.0625, p3 0.078125, p5 0.0625\n",
      "2017-09-06T19:00:16.293271: step 41, loss 15.2104, p1 0.109375, p3 0.0520833, p5 0.05625\n",
      "2017-09-06T19:00:17.955607: step 42, loss 16.2219, p1 0.109375, p3 0.0625, p5 0.0625\n",
      "2017-09-06T19:00:19.657003: step 43, loss 15.438, p1 0.09375, p3 0.0833333, p5 0.05625\n",
      "2017-09-06T19:00:21.335985: step 44, loss 16.3017, p1 0.0625, p3 0.0625, p5 0.05625\n",
      "2017-09-06T19:00:22.992295: step 45, loss 15.8305, p1 0.078125, p3 0.0625, p5 0.059375\n",
      "2017-09-06T19:00:24.669625: step 46, loss 16.9116, p1 0.09375, p3 0.0625, p5 0.059375\n",
      "2017-09-06T19:00:26.346093: step 47, loss 14.4349, p1 0.078125, p3 0.046875, p5 0.05\n",
      "2017-09-06T19:00:28.041613: step 48, loss 17.5495, p1 0.09375, p3 0.0989583, p5 0.0875\n",
      "2017-09-06T19:00:29.745365: step 49, loss 16.2509, p1 0.109375, p3 0.0885417, p5 0.08125\n",
      "2017-09-06T19:00:31.528710: step 50, loss 15.8751, p1 0.109375, p3 0.0833333, p5 0.059375\n",
      "2017-09-06T19:00:33.232038: step 51, loss 18.5418, p1 0.125, p3 0.0885417, p5 0.078125\n",
      "2017-09-06T19:00:34.933789: step 52, loss 17.8855, p1 0.09375, p3 0.078125, p5 0.059375\n",
      "2017-09-06T19:00:36.627709: step 53, loss 18.313, p1 0.078125, p3 0.0625, p5 0.053125\n",
      "2017-09-06T19:00:38.308647: step 54, loss 17.8134, p1 0.03125, p3 0.0677083, p5 0.06875\n",
      "2017-09-06T19:00:40.016132: step 55, loss 18.4896, p1 0.109375, p3 0.0677083, p5 0.06875\n",
      "2017-09-06T19:00:41.709611: step 56, loss 18.2829, p1 0.078125, p3 0.0885417, p5 0.078125\n",
      "2017-09-06T19:00:43.503587: step 57, loss 17.7876, p1 0.078125, p3 0.0677083, p5 0.065625\n",
      "2017-09-06T19:00:45.214848: step 58, loss 17.754, p1 0.140625, p3 0.0729167, p5 0.071875\n",
      "2017-09-06T19:00:46.926870: step 59, loss 18.7354, p1 0.09375, p3 0.0677083, p5 0.075\n",
      "2017-09-06T19:00:48.636474: step 60, loss 18.8399, p1 0.140625, p3 0.0572917, p5 0.05625\n",
      "2017-09-06T19:00:50.319926: step 61, loss 20.2056, p1 0.109375, p3 0.0885417, p5 0.08125\n",
      "2017-09-06T19:00:52.017952: step 62, loss 21.1702, p1 0.078125, p3 0.0625, p5 0.0625\n",
      "2017-09-06T19:00:53.701194: step 63, loss 21.9107, p1 0.09375, p3 0.078125, p5 0.06875\n",
      "2017-09-06T19:00:55.395919: step 64, loss 21.2883, p1 0.0625, p3 0.0572917, p5 0.053125\n",
      "2017-09-06T19:00:57.071226: step 65, loss 21.8656, p1 0.0625, p3 0.0625, p5 0.065625\n",
      "2017-09-06T19:00:58.770953: step 66, loss 23.7665, p1 0.109375, p3 0.0885417, p5 0.06875\n",
      "2017-09-06T19:01:00.456507: step 67, loss 23.2035, p1 0.046875, p3 0.0833333, p5 0.065625\n",
      "2017-09-06T19:01:02.149933: step 68, loss 20.9344, p1 0.109375, p3 0.0833333, p5 0.071875\n",
      "2017-09-06T19:01:03.879488: step 69, loss 23.959, p1 0.109375, p3 0.104167, p5 0.08125\n",
      "2017-09-06T19:01:05.584617: step 70, loss 23.245, p1 0.078125, p3 0.0625, p5 0.075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-06T19:01:07.280976: step 71, loss 22.4305, p1 0.046875, p3 0.0677083, p5 0.059375\n",
      "2017-09-06T19:01:08.976714: step 72, loss 22.236, p1 0.03125, p3 0.046875, p5 0.040625\n",
      "2017-09-06T19:01:10.627901: step 73, loss 24.4546, p1 0.078125, p3 0.0625, p5 0.0625\n",
      "2017-09-06T19:01:12.284164: step 74, loss 22.6739, p1 0.109375, p3 0.078125, p5 0.065625\n",
      "2017-09-06T19:01:13.942330: step 75, loss 25.4781, p1 0.09375, p3 0.0833333, p5 0.090625\n",
      "2017-09-06T19:01:15.626964: step 76, loss 29.9019, p1 0.078125, p3 0.0833333, p5 0.075\n",
      "2017-09-06T19:01:17.291963: step 77, loss 31.451, p1 0.109375, p3 0.0885417, p5 0.0875\n",
      "2017-09-06T19:01:18.965413: step 78, loss 28.0581, p1 0.09375, p3 0.0729167, p5 0.059375\n",
      "2017-09-06T19:01:20.629521: step 79, loss 29.4003, p1 0.125, p3 0.078125, p5 0.0625\n",
      "2017-09-06T19:01:22.313177: step 80, loss 31.7548, p1 0.09375, p3 0.09375, p5 0.084375\n",
      "2017-09-06T19:01:24.014969: step 81, loss 32.8082, p1 0.09375, p3 0.078125, p5 0.071875\n",
      "2017-09-06T19:01:25.709890: step 82, loss 29.8645, p1 0.140625, p3 0.078125, p5 0.075\n",
      "2017-09-06T19:01:27.386683: step 83, loss 31.8791, p1 0.0625, p3 0.0625, p5 0.071875\n",
      "2017-09-06T19:01:29.078036: step 84, loss 37.252, p1 0.09375, p3 0.0677083, p5 0.0625\n",
      "2017-09-06T19:01:30.804085: step 85, loss 34.3132, p1 0.125, p3 0.0885417, p5 0.075\n",
      "2017-09-06T19:01:32.535771: step 86, loss 39.5897, p1 0.0625, p3 0.0833333, p5 0.075\n",
      "2017-09-06T19:01:34.228756: step 87, loss 33.6857, p1 0.09375, p3 0.0677083, p5 0.065625\n",
      "2017-09-06T19:01:35.907405: step 88, loss 39.3954, p1 0.078125, p3 0.0677083, p5 0.071875\n",
      "2017-09-06T19:01:37.577001: step 89, loss 38.9631, p1 0.0625, p3 0.0833333, p5 0.078125\n",
      "2017-09-06T19:01:39.232197: step 90, loss 40.5482, p1 0.046875, p3 0.0833333, p5 0.090625\n",
      "2017-09-06T19:01:40.908913: step 91, loss 39.1137, p1 0.0625, p3 0.0833333, p5 0.071875\n",
      "2017-09-06T19:01:42.581886: step 92, loss 41.2389, p1 0.0625, p3 0.078125, p5 0.071875\n",
      "2017-09-06T19:01:44.285854: step 93, loss 43.6178, p1 0.078125, p3 0.104167, p5 0.084375\n",
      "2017-09-06T19:01:46.017292: step 94, loss 45.0457, p1 0.109375, p3 0.0885417, p5 0.06875\n",
      "2017-09-06T19:01:47.770115: step 95, loss 45.0904, p1 0.078125, p3 0.0625, p5 0.065625\n",
      "2017-09-06T19:01:49.460916: step 96, loss 42.4501, p1 0.078125, p3 0.0833333, p5 0.084375\n",
      "2017-09-06T19:01:51.155743: step 97, loss 45.9462, p1 0.0625, p3 0.0572917, p5 0.05625\n",
      "2017-09-06T19:01:52.834275: step 98, loss 47.9011, p1 0.09375, p3 0.09375, p5 0.078125\n",
      "2017-09-06T19:01:54.520677: step 99, loss 49.8316, p1 0.125, p3 0.0833333, p5 0.075\n",
      "2017-09-06T19:01:56.210060: step 100, loss 46.1722, p1 0.125, p3 0.078125, p5 0.075\n",
      "\n",
      "Evaluation:\n",
      "2017-09-06T19:02:00.641766: step 100, loss 183.339, p1 0.330537, p3 0.189597, p5 0.145638\n",
      "\n",
      "Saved model checkpoint to /home/cloud-user/code/network_embedding/runs/1504724346/checkpoints/model-100\n",
      "\n",
      "2017-09-06T19:02:02.480358: step 101, loss 51.8202, p1 0.0625, p3 0.0677083, p5 0.05625\n",
      "2017-09-06T19:02:04.178065: step 102, loss 48.9397, p1 0.078125, p3 0.0677083, p5 0.071875\n",
      "2017-09-06T19:02:05.842643: step 103, loss 52.7191, p1 0.171875, p3 0.09375, p5 0.078125\n",
      "2017-09-06T19:02:07.540633: step 104, loss 50.3349, p1 0.09375, p3 0.0833333, p5 0.078125\n",
      "2017-09-06T19:02:09.209378: step 105, loss 54.6688, p1 0.0625, p3 0.0677083, p5 0.071875\n",
      "2017-09-06T19:02:10.850262: step 106, loss 54.0399, p1 0.140625, p3 0.0833333, p5 0.065625\n",
      "2017-09-06T19:02:12.551672: step 107, loss 53.2981, p1 0.015625, p3 0.0520833, p5 0.05625\n",
      "2017-09-06T19:02:14.225460: step 108, loss 63.4944, p1 0.015625, p3 0.0729167, p5 0.071875\n",
      "2017-09-06T19:02:15.907423: step 109, loss 60.2715, p1 0.15625, p3 0.09375, p5 0.075\n",
      "2017-09-06T19:02:17.566866: step 110, loss 68.6358, p1 0.0625, p3 0.0729167, p5 0.075\n",
      "2017-09-06T19:02:19.254203: step 111, loss 65.6993, p1 0.046875, p3 0.0833333, p5 0.06875\n",
      "2017-09-06T19:02:20.938700: step 112, loss 64.7045, p1 0.078125, p3 0.078125, p5 0.065625\n",
      "2017-09-06T19:02:22.616640: step 113, loss 59.7374, p1 0.078125, p3 0.0572917, p5 0.071875\n",
      "2017-09-06T19:02:24.290066: step 114, loss 69.5712, p1 0.15625, p3 0.0885417, p5 0.075\n",
      "2017-09-06T19:02:25.969917: step 115, loss 66.9383, p1 0.125, p3 0.0677083, p5 0.053125\n",
      "2017-09-06T19:02:27.668706: step 116, loss 76.9265, p1 0.109375, p3 0.0885417, p5 0.075\n",
      "2017-09-06T19:02:29.379462: step 117, loss 68.4926, p1 0.0625, p3 0.0833333, p5 0.065625\n",
      "2017-09-06T19:02:31.058532: step 118, loss 73.3094, p1 0.09375, p3 0.078125, p5 0.084375\n",
      "2017-09-06T19:02:32.821854: step 119, loss 71.1624, p1 0.09375, p3 0.078125, p5 0.084375\n",
      "2017-09-06T19:02:34.521514: step 120, loss 71.197, p1 0.046875, p3 0.0833333, p5 0.0625\n",
      "2017-09-06T19:02:36.253512: step 121, loss 76.6955, p1 0.0625, p3 0.0729167, p5 0.059375\n",
      "2017-09-06T19:02:37.978095: step 122, loss 75.6337, p1 0.078125, p3 0.0833333, p5 0.078125\n",
      "2017-09-06T19:02:39.740575: step 123, loss 79.265, p1 0.09375, p3 0.078125, p5 0.08125\n",
      "2017-09-06T19:02:41.520373: step 124, loss 89.752, p1 0.09375, p3 0.09375, p5 0.078125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1391a04d3d5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-1391a04d3d5f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch_binary, y_batch_labels)\u001b[0m\n\u001b[1;32m     79\u001b[0m             _, step, summaries, loss, p1, p3, p5 = sess.run(\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/network_embedding/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/network_embedding/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/network_embedding/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/code/network_embedding/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/network_embedding/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = KimCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=num_classes,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and precision\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        p1 = tf.summary.scalar(\"p1\", cnn.p1)\n",
    "        p3 = tf.summary.scalar(\"p3\", cnn.p3)\n",
    "        p5 = tf.summary.scalar(\"p5\", cnn.p5)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, p1, p3, p5, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, p1, p3, p5])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch_binary, y_batch_labels):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y_binary: y_batch_binary,\n",
    "              cnn.input_y_labels: label_lists_to_sparse_tuple(y_batch_labels, num_classes),  # needs some conversion\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, p1, p3, p5 = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.p1, cnn.p3, cnn.p5],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(time_str, step, loss, p1, p3, p5))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch_binary, y_batch_labels, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y_binary: y_batch_binary,\n",
    "              cnn.input_y_labels: label_lists_to_sparse_tuple(y_batch_labels, num_classes),  # needs some conversion\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, p1, p3, p5 = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.p1, cnn.p3, cnn.p5],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(time_str, step, loss, p1, p3, p5))            \n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train_binary, y_train_labels)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch_binary, y_train_labels = zip(*batch)\n",
    "            train_step(x_batch, y_batch_binary, y_train_labels)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev_binary, y_dev_labels, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
