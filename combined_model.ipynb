{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.contrib import learn\n",
    "from itertools import repeat, chain\n",
    "\n",
    "from kim_cnn import KimCNN\n",
    "from word2vec import Word2Vec\n",
    "from combined import Combined\n",
    "from eval_helpers import label_lists_to_sparse_tuple\n",
    "from data_helpers import batch_iter, RWBatchGenerator\n",
    "from tf_helpers import save_embedding_for_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string('data_dir', 'data/stackexchange/datascience/', 'directory of dataset')\n",
    "tf.flags.DEFINE_integer('tag_freq_threshold', 5, 'minimum frequency of a tag')\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_float(\"max_document_length\", 2000, \"Maximum length of document, exceeding part is truncated\")\n",
    "\n",
    "# Architecutural parameters for KimCNN\n",
    "\n",
    "tf.flags.DEFINE_string(\"loss_function\", 'sigmoid', \"loss function: (softmax|sigmoid) (Default: sigmoid)\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer(\"dw_batch_size\", 128, \"Batch Size for deep walk model (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"dw_skip_window\", 3, \"How many words to consider left and right. (default: 3)\")\n",
    "tf.flags.DEFINE_integer(\"dw_num_skips\", 4, \"How many times to reuse an input to generate a label. (default: 4)\")\n",
    "tf.flags.DEFINE_integer(\"dw_embedding_size\", 128, \"Dimensionality of node embedding. (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"dw_num_negative_samples\", 64, \"Number of negative examples to sample. (default: 64)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global training parameter\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DATA_DIR=data/stackexchange/datascience/\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "DW_BATCH_SIZE=128\n",
      "DW_EMBEDDING_SIZE=128\n",
      "DW_NUM_NEGATIVE_SAMPLES=64\n",
      "DW_NUM_SKIPS=4\n",
      "DW_SKIP_WINDOW=3\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "LOSS_FUNCTION=sigmoid\n",
      "MAX_DOCUMENT_LENGTH=2000\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "TAG_FREQ_THRESHOLD=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "data_dir = FLAGS.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 4630/515\n",
      "num of classes: 328\n"
     ]
    }
   ],
   "source": [
    "# load text data and label information\n",
    "\n",
    "text_path = os.path.join(data_dir, \"input_text.csv\")\n",
    "tdf = pd.read_csv(text_path, header=None)\n",
    "x_text = tdf[1]\n",
    "\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(FLAGS.max_document_length)\n",
    "X = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "node_ids = np.arange(X.shape[0])\n",
    "\n",
    "# load train/test data\n",
    "Y_labels = pkl.load(open(os.path.join(data_dir, \"Y.pkl\"), 'rb'))\n",
    "\n",
    "\n",
    "size = sum(len(ls) for ls in Y_labels)\n",
    "row_indx = list(chain(*[list(repeat(i, len(ls))) for i, ls in enumerate(Y_labels)]))\n",
    "col_indx = list(chain(*Y_labels))\n",
    "Y_binary = csr_matrix((np.ones(size), (row_indx, col_indx)),\n",
    "                      shape=(len(Y_labels), len(set(col_indx)))).toarray()\n",
    "\n",
    "\n",
    "# split data\n",
    "x_train, x_dev, y_train_binary, y_dev_binary, y_train_labels, y_dev_labels, train_node_ids, dev_node_ids = train_test_split(\n",
    "    X, Y_binary, Y_labels, node_ids, train_size=1 - FLAGS.dev_sample_percentage, random_state=42)\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev)))\n",
    "\n",
    "num_classes = y_train_binary.shape[1]\n",
    "print(\"num of classes: {:d}\".format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load node embedding data\n",
    "\n",
    "walks = RWBatchGenerator.read_walks(\"{}/random_walks.txt\".format(data_dir))\n",
    "\n",
    "vocabulary_size = len(set(itertools.chain(*walks)))\n",
    "\n",
    "dw_data_generator = RWBatchGenerator(\n",
    "    walks, FLAGS.dw_batch_size, FLAGS.dw_num_skips, FLAGS.dw_skip_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use sigmoid xentropy\n",
      "Writing to /home/cloud-user/code/network_embedding/runs/combined\n",
      "\n",
      "label step\n",
      "2017-09-22T21:06:59.970462: step 0, loss 1.90154, p1 0.015625, p3 0.00520833, p5 0.00625\n",
      "graph step\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimensions of inputs should match: shape[0] = [1,384] vs. shape[1] = [0,128]\n\t [[Node: combined/output/input_concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](kim_cnn/dropout/dropout/mul, combined/output/node_embeddings, combined/output/input_concat/axis)]]\n\nCaused by op 'combined/output/input_concat', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-beb025b6534f>\", line 30, in <module>\n    model = Combined(cnn, dw)\n  File \"/home/cloud-user/code/network_embedding/combined.py\", line 14, in __init__\n    self.add_output()\n  File \"/home/cloud-user/code/network_embedding/combined.py\", line 36, in add_output\n    1, name=\"input_concat\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1066, in concat\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 493, in _concat_v2\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [1,384] vs. shape[1] = [0,128]\n\t [[Node: combined/output/input_concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](kim_cnn/dropout/dropout/mul, combined/output/node_embeddings, combined/output/input_concat/axis)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [1,384] vs. shape[1] = [0,128]\n\t [[Node: combined/output/input_concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](kim_cnn/dropout/dropout/mul, combined/output/node_embeddings, combined/output/input_concat/axis)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-beb025b6534f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdw_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graph step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrain_graph_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# one step for graph training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-beb025b6534f>\u001b[0m in \u001b[0;36mtrain_graph_step\u001b[0;34m(x_batch, batch_labels, writer)\u001b[0m\n\u001b[1;32m    121\u001b[0m             _, step, summaries, label_loss = sess.run(\n\u001b[1;32m    122\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mgraph_train_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             print(\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [1,384] vs. shape[1] = [0,128]\n\t [[Node: combined/output/input_concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](kim_cnn/dropout/dropout/mul, combined/output/node_embeddings, combined/output/input_concat/axis)]]\n\nCaused by op 'combined/output/input_concat', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-21-beb025b6534f>\", line 30, in <module>\n    model = Combined(cnn, dw)\n  File \"/home/cloud-user/code/network_embedding/combined.py\", line 14, in __init__\n    self.add_output()\n  File \"/home/cloud-user/code/network_embedding/combined.py\", line 36, in add_output\n    1, name=\"input_concat\")\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1066, in concat\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 493, in _concat_v2\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [1,384] vs. shape[1] = [0,128]\n\t [[Node: combined/output/input_concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](kim_cnn/dropout/dropout/mul, combined/output/node_embeddings, combined/output/input_concat/axis)]]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        with tf.name_scope('kim_cnn'):\n",
    "            cnn = KimCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=num_classes,\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                loss_function=FLAGS.loss_function,\n",
    "                redefine_output_layer=True)\n",
    "\n",
    "        with tf.name_scope('dw'):\n",
    "            dw = Word2Vec(FLAGS.dw_num_negative_samples,\n",
    "                          vocabulary_size,\n",
    "                          FLAGS.dw_embedding_size)\n",
    "        \n",
    "        with tf.name_scope('combined'):\n",
    "            model = Combined(cnn, dw)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        label_train_op = tf.train.AdamOptimizer(1e-3).minimize(model.label_loss)        \n",
    "        graph_train_op = tf.train.GradientDescentOptimizer(1.0).minimize(model.graph_loss)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", 'combined'))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        if tf.gfile.Exists(out_dir):\n",
    "            tf.gfile.DeleteRecursively(out_dir)\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "        \n",
    "        # Summaries for loss and precision\n",
    "        label_loss_summary = tf.summary.scalar(\"label_loss\", model.label_loss)\n",
    "        graph_loss_summary = tf.summary.scalar(\"graph_loss\", model.graph_loss)        \n",
    "        p1 = tf.summary.scalar(\"p1\", model.p1)\n",
    "        p3 = tf.summary.scalar(\"p3\", model.p3)\n",
    "        p5 = tf.summary.scalar(\"p5\", model.p5)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([label_loss_summary, graph_loss_summary,\n",
    "                                             p1, p3, p5])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([label_loss_summary, graph_loss_summary,\n",
    "                                           p1, p3, p5])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())        \n",
    "        \n",
    "        def train_label_step(x_batch, y_batch_binary, y_batch_labels, node_ids, writer):\n",
    "            \"\"\"\n",
    "            one training step for the label part\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              model.cnn.input_x: x_batch,\n",
    "              model.cnn.input_y_binary: y_batch_binary,\n",
    "              model.cnn.input_y_labels: label_lists_to_sparse_tuple(\n",
    "                  y_batch_labels, num_classes),  # needs some conversion\n",
    "              model.node_ids: node_ids, # node ids\n",
    "              model.cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "                \n",
    "              # the following is in vain\n",
    "              # tf requires all placeholder to be provided some value                \n",
    "              model.dw.train_inputs: [0],\n",
    "              model.dw.train_labels: [[0]],\n",
    "            }\n",
    "            _, step, summaries, label_loss, p1, p3, p5 = sess.run(\n",
    "                [label_train_op, global_step, train_summary_op, model.label_loss, model.p1, model.p3, model.p5],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
    "                time_str, step, label_loss, p1, p3, p5))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def train_graph_step(x_batch, batch_labels, writer):\n",
    "            \"\"\"\n",
    "            one training step for the graph part\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              model.dw.train_inputs: x_batch,\n",
    "              model.dw.train_labels: np.expand_dims(np.array(batch_labels), -1),\n",
    "                \n",
    "              # the following is in vain\n",
    "              # tf requires all placeholder to be provided some value\n",
    "              model.cnn.input_x: list(vocab_processor.transform([\"asdfkjahdkfhakslfh\"])),  # non-sense stuff\n",
    "              model.cnn.input_y_binary: [[0] * num_classes],  # with no label\n",
    "              model.cnn.input_y_labels: label_lists_to_sparse_tuple(\n",
    "                  [[0]], num_classes),  # needs some conversion\n",
    "              model.node_ids: [], # node ids\n",
    "              model.cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "                \n",
    "            }\n",
    "            _, step, summaries, label_loss = sess.run(\n",
    "                [graph_train_op, global_step, train_summary_op, model.graph_loss],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
    "                time_str, step, label_loss, p1, p3, p5))\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "            \n",
    "        def dev_step(x_batch, y_batch_binary, y_batch_labels, node_ids, writer):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              model.cnn.input_x: x_batch,\n",
    "              model.cnn.input_y_binary: y_batch_binary,\n",
    "              model.cnn.input_y_labels: label_lists_to_sparse_tuple(\n",
    "                  y_batch_labels, num_classes),  # needs some conversion\n",
    "              model.node_ids: node_ids, # node ids                \n",
    "              model.cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, label_loss, p1, p3, p5 = sess.run(\n",
    "                [global_step, dev_summary_op, model.label_loss, model.p1, model.p3, model.p5],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
    "                time_str, step, label_loss, p1, p3, p5))\n",
    "            \n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train_binary, y_train_labels, train_node_ids)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "        for batch in batches:\n",
    "            # train label part\n",
    "            x_batch, y_batch_binary, y_train_labels, x_node_ids = zip(*batch)\n",
    "            print('label step')\n",
    "            train_label_step(x_batch, y_batch_binary, y_train_labels, x_node_ids, train_summary_writer)\n",
    "            current_step = tf.train.global_step(sess, global_step)  # one step for label training\n",
    "            \n",
    "            # train graph part\n",
    "            batch_inputs, batch_labels = dw_data_generator.next_batch()\n",
    "            print('graph step')\n",
    "            train_graph_step(batch_inputs, batch_labels, train_summary_writer)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)  # one step for graph training\n",
    "            \n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev_binary, y_dev_labels, dev_node_ids, dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
