{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.contrib import learn\n",
    "from itertools import repeat, chain\n",
    "\n",
    "from kim_cnn import KimCNN\n",
    "from word2vec import Word2Vec\n",
    "from combined import Combined\n",
    "from eval_helpers import label_lists_to_sparse_tuple\n",
    "from data_helpers import batch_iter, RWBatchGenerator\n",
    "from tf_helpers import save_embedding_for_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_string('data_dir', 'data/stackexchange/datascience/', 'directory of dataset')\n",
    "tf.flags.DEFINE_integer('tag_freq_threshold', 5, 'minimum frequency of a tag')\n",
    "\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_float(\"max_document_length\", 2000, \"Maximum length of document, exceeding part is truncated\")\n",
    "\n",
    "# Architecutural parameters for KimCNN\n",
    "\n",
    "tf.flags.DEFINE_string(\"loss_function\", 'sigmoid', \"loss function: (softmax|sigmoid) (Default: sigmoid)\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.flags.DEFINE_integer(\"dw_batch_size\", 128, \"Batch Size for deep walk model (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"dw_skip_window\", 3, \"How many words to consider left and right. (default: 3)\")\n",
    "tf.flags.DEFINE_integer(\"dw_num_skips\", 4, \"How many times to reuse an input to generate a label. (default: 4)\")\n",
    "tf.flags.DEFINE_integer(\"dw_embedding_size\", 128, \"Dimensionality of node embedding. (default: 128)\")\n",
    "tf.flags.DEFINE_integer(\"dw_num_negative_samples\", 64, \"Number of negative examples to sample. (default: 64)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global training parameter\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=1000\n",
      "DATA_DIR=data/stackexchange/datascience/\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "DW_BATCH_SIZE=128\n",
      "DW_EMBEDDING_SIZE=128\n",
      "DW_NUM_NEGATIVE_SAMPLES=64\n",
      "DW_NUM_SKIPS=4\n",
      "DW_SKIP_WINDOW=3\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "LOSS_FUNCTION=sigmoid\n",
      "MAX_DOCUMENT_LENGTH=2000\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "TAG_FREQ_THRESHOLD=5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "data_dir = FLAGS.data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 4630/515\n",
      "num of classes: 328\n"
     ]
    }
   ],
   "source": [
    "# load text data and label information\n",
    "\n",
    "text_path = os.path.join(data_dir, \"input_text.csv\")\n",
    "tdf = pd.read_csv(text_path, header=None)\n",
    "x_text = tdf[1]\n",
    "\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(FLAGS.max_document_length)\n",
    "X = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "node_ids = np.arange(X.shape[0])\n",
    "\n",
    "# load train/test data\n",
    "Y_labels = pkl.load(open(os.path.join(data_dir, \"Y.pkl\"), 'rb'))\n",
    "\n",
    "\n",
    "size = sum(len(ls) for ls in Y_labels)\n",
    "row_indx = list(chain(*[list(repeat(i, len(ls))) for i, ls in enumerate(Y_labels)]))\n",
    "col_indx = list(chain(*Y_labels))\n",
    "Y_binary = csr_matrix((np.ones(size), (row_indx, col_indx)),\n",
    "                      shape=(len(Y_labels), len(set(col_indx)))).toarray()\n",
    "\n",
    "\n",
    "# split data\n",
    "x_train, x_dev, y_train_binary, y_dev_binary, y_train_labels, y_dev_labels, train_node_ids, dev_node_ids = train_test_split(\n",
    "    X, Y_binary, Y_labels, node_ids, train_size=1 - FLAGS.dev_sample_percentage, random_state=42)\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(x_train), len(x_dev)))\n",
    "\n",
    "num_classes = y_train_binary.shape[1]\n",
    "print(\"num of classes: {:d}\".format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load node embedding data\n",
    "\n",
    "walks = RWBatchGenerator.read_walks(\"{}/random_walks.txt\".format(data_dir))\n",
    "\n",
    "vocabulary_size = len(set(itertools.chain(*walks)))\n",
    "\n",
    "dw_data_generator = RWBatchGenerator(\n",
    "    walks, FLAGS.dw_batch_size, FLAGS.dw_num_skips, FLAGS.dw_skip_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn: <class 'kim_cnn.KimCNN'>\n",
      "dw: <class 'word2vec.Word2Vec'>\n",
      "use sigmoid xentropy\n",
      "Writing to /home/cloud-user/code/network_embedding/runs/combined\n",
      "\n",
      "2017-09-23T16:13:52.641463: step 0, label loss 1.85806, p1 0.015625, p3 0.015625, p5 0.0125\n",
      "2017-09-23T16:13:52.686052: step 0, graph loss 210.697\n",
      "\n",
      "Evaluation:\n",
      "2017-09-23T16:13:56.735411: step 0, label loss 0.938104, p1 0.023301, p3 0.00906149, p5 0.00776699\n",
      "\n",
      "Saved model checkpoint to /home/cloud-user/code/network_embedding/runs/combined/checkpoints/model-0\n",
      "\n",
      "2017-09-23T16:13:58.539058: step 1, label loss 1.35152, p1 0, p3 0, p5 0\n",
      "2017-09-23T16:13:58.585387: step 1, graph loss 194.461\n",
      "2017-09-23T16:14:00.221543: step 2, label loss 0.932381, p1 0, p3 0, p5 0\n",
      "2017-09-23T16:14:00.264123: step 2, graph loss 179.424\n",
      "2017-09-23T16:14:01.924874: step 3, label loss 0.646928, p1 0.015625, p3 0.00520833, p5 0.00625\n",
      "2017-09-23T16:14:01.967379: step 3, graph loss 210.598\n",
      "2017-09-23T16:14:03.616980: step 4, label loss 0.432819, p1 0.015625, p3 0.00520833, p5 0.009375\n",
      "2017-09-23T16:14:03.666980: step 4, graph loss 170.531\n",
      "2017-09-23T16:14:05.311002: step 5, label loss 0.274923, p1 0, p3 0, p5 0.0125\n",
      "2017-09-23T16:14:05.358298: step 5, graph loss 173.498\n",
      "2017-09-23T16:14:07.006911: step 6, label loss 0.20594, p1 0, p3 0.015625, p5 0.0125\n",
      "2017-09-23T16:14:07.057388: step 6, graph loss 202.419\n",
      "2017-09-23T16:14:08.698082: step 7, label loss 0.153021, p1 0.03125, p3 0.0260417, p5 0.028125\n",
      "2017-09-23T16:14:08.744824: step 7, graph loss 179.9\n",
      "2017-09-23T16:14:10.387460: step 8, label loss 0.113114, p1 0.015625, p3 0.0104167, p5 0.01875\n",
      "2017-09-23T16:14:10.432943: step 8, graph loss 145.731\n",
      "2017-09-23T16:14:12.078009: step 9, label loss 0.09473, p1 0, p3 0.0104167, p5 0.015625\n",
      "2017-09-23T16:14:12.126233: step 9, graph loss 197.999\n",
      "2017-09-23T16:14:13.789569: step 10, label loss 0.0831832, p1 0.03125, p3 0.0416667, p5 0.03125\n",
      "2017-09-23T16:14:13.847550: step 10, graph loss 166.115\n",
      "2017-09-23T16:14:15.547008: step 11, label loss 0.0695652, p1 0.078125, p3 0.0520833, p5 0.065625\n",
      "2017-09-23T16:14:15.594607: step 11, graph loss 180.561\n",
      "2017-09-23T16:14:17.225860: step 12, label loss 0.0697251, p1 0.09375, p3 0.0625, p5 0.0625\n",
      "2017-09-23T16:14:17.270475: step 12, graph loss 176.24\n",
      "2017-09-23T16:14:18.875839: step 13, label loss 0.0707805, p1 0.140625, p3 0.0989583, p5 0.065625\n",
      "2017-09-23T16:14:18.927420: step 13, graph loss 162.017\n",
      "2017-09-23T16:14:20.556378: step 14, label loss 0.0725975, p1 0.203125, p3 0.114583, p5 0.096875\n",
      "2017-09-23T16:14:20.601447: step 14, graph loss 185.247\n",
      "2017-09-23T16:14:22.220519: step 15, label loss 0.0678972, p1 0.109375, p3 0.0677083, p5 0.08125\n",
      "2017-09-23T16:14:22.268688: step 15, graph loss 165.61\n",
      "2017-09-23T16:14:23.924171: step 16, label loss 0.0623232, p1 0.171875, p3 0.130208, p5 0.10625\n",
      "2017-09-23T16:14:23.973232: step 16, graph loss 178.961\n",
      "2017-09-23T16:14:25.652759: step 17, label loss 0.0704016, p1 0.140625, p3 0.109375, p5 0.084375\n",
      "2017-09-23T16:14:25.703234: step 17, graph loss 171.459\n",
      "2017-09-23T16:14:27.345292: step 18, label loss 0.0701512, p1 0.171875, p3 0.130208, p5 0.10625\n",
      "2017-09-23T16:14:27.390025: step 18, graph loss 164.914\n",
      "2017-09-23T16:14:28.991759: step 19, label loss 0.0692405, p1 0.171875, p3 0.130208, p5 0.11875\n",
      "2017-09-23T16:14:29.038143: step 19, graph loss 166.328\n",
      "2017-09-23T16:14:30.733705: step 20, label loss 0.0691439, p1 0.140625, p3 0.0989583, p5 0.0875\n",
      "2017-09-23T16:14:30.783031: step 20, graph loss 190.389\n",
      "2017-09-23T16:14:32.473523: step 21, label loss 0.0675233, p1 0.140625, p3 0.0989583, p5 0.075\n",
      "2017-09-23T16:14:32.517258: step 21, graph loss 148.751\n",
      "2017-09-23T16:14:34.197964: step 22, label loss 0.0710272, p1 0.25, p3 0.1875, p5 0.153125\n",
      "2017-09-23T16:14:34.239683: step 22, graph loss 143.802\n",
      "2017-09-23T16:14:35.904938: step 23, label loss 0.0712614, p1 0.171875, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:14:35.950618: step 23, graph loss 145.015\n",
      "2017-09-23T16:14:37.644864: step 24, label loss 0.0622302, p1 0.15625, p3 0.1875, p5 0.159375\n",
      "2017-09-23T16:14:37.693819: step 24, graph loss 181.335\n",
      "2017-09-23T16:14:39.345105: step 25, label loss 0.0693677, p1 0.21875, p3 0.125, p5 0.109375\n",
      "2017-09-23T16:14:39.390777: step 25, graph loss 183.568\n",
      "2017-09-23T16:14:41.069973: step 26, label loss 0.0689489, p1 0.109375, p3 0.109375, p5 0.096875\n",
      "2017-09-23T16:14:41.119130: step 26, graph loss 163.53\n",
      "2017-09-23T16:14:42.812933: step 27, label loss 0.065584, p1 0.125, p3 0.119792, p5 0.1\n",
      "2017-09-23T16:14:42.861234: step 27, graph loss 187.963\n",
      "2017-09-23T16:14:44.549662: step 28, label loss 0.066847, p1 0.140625, p3 0.130208, p5 0.109375\n",
      "2017-09-23T16:14:44.598242: step 28, graph loss 137.881\n",
      "2017-09-23T16:14:46.231483: step 29, label loss 0.0645327, p1 0.125, p3 0.125, p5 0.1125\n",
      "2017-09-23T16:14:46.284198: step 29, graph loss 155.755\n",
      "2017-09-23T16:14:47.923248: step 30, label loss 0.0721209, p1 0.140625, p3 0.125, p5 0.103125\n",
      "2017-09-23T16:14:47.972716: step 30, graph loss 136.08\n",
      "2017-09-23T16:14:49.632759: step 31, label loss 0.0816596, p1 0.125, p3 0.125, p5 0.09375\n",
      "2017-09-23T16:14:49.680027: step 31, graph loss 165.523\n",
      "2017-09-23T16:14:51.317596: step 32, label loss 0.0701407, p1 0.265625, p3 0.15625, p5 0.11875\n",
      "2017-09-23T16:14:51.362413: step 32, graph loss 173.879\n",
      "2017-09-23T16:14:53.018117: step 33, label loss 0.075855, p1 0.078125, p3 0.119792, p5 0.1125\n",
      "2017-09-23T16:14:53.069508: step 33, graph loss 169.862\n",
      "2017-09-23T16:14:54.820552: step 34, label loss 0.0756669, p1 0.15625, p3 0.109375, p5 0.09375\n",
      "2017-09-23T16:14:54.869116: step 34, graph loss 148.047\n",
      "2017-09-23T16:14:56.526541: step 35, label loss 0.0719539, p1 0.09375, p3 0.119792, p5 0.090625\n",
      "2017-09-23T16:14:56.572727: step 35, graph loss 125.218\n",
      "2017-09-23T16:14:58.208796: step 36, label loss 0.0711911, p1 0.09375, p3 0.078125, p5 0.075\n",
      "2017-09-23T16:14:58.256701: step 36, graph loss 168.172\n",
      "2017-09-23T16:14:59.874887: step 37, label loss 0.0696955, p1 0.125, p3 0.130208, p5 0.11875\n",
      "2017-09-23T16:14:59.918808: step 37, graph loss 117.303\n",
      "2017-09-23T16:15:01.593173: step 38, label loss 0.0597126, p1 0.140625, p3 0.114583, p5 0.084375\n",
      "2017-09-23T16:15:01.638973: step 38, graph loss 126.867\n",
      "2017-09-23T16:15:03.253100: step 39, label loss 0.0719272, p1 0.09375, p3 0.0833333, p5 0.084375\n",
      "2017-09-23T16:15:03.296918: step 39, graph loss 133.278\n",
      "2017-09-23T16:15:04.957407: step 40, label loss 0.0564678, p1 0.109375, p3 0.0885417, p5 0.103125\n",
      "2017-09-23T16:15:05.007561: step 40, graph loss 146.28\n",
      "2017-09-23T16:15:06.642159: step 41, label loss 0.0633968, p1 0.125, p3 0.114583, p5 0.08125\n",
      "2017-09-23T16:15:06.689061: step 41, graph loss 148.22\n",
      "2017-09-23T16:15:08.326606: step 42, label loss 0.0619831, p1 0.171875, p3 0.0989583, p5 0.06875\n",
      "2017-09-23T16:15:08.374799: step 42, graph loss 181.362\n",
      "2017-09-23T16:15:10.007561: step 43, label loss 0.0624576, p1 0.140625, p3 0.0989583, p5 0.09375\n",
      "2017-09-23T16:15:10.059379: step 43, graph loss 152.143\n",
      "2017-09-23T16:15:11.706111: step 44, label loss 0.0657841, p1 0.171875, p3 0.109375, p5 0.084375\n",
      "2017-09-23T16:15:11.755155: step 44, graph loss 149.723\n",
      "2017-09-23T16:15:13.396833: step 45, label loss 0.0636111, p1 0.109375, p3 0.104167, p5 0.084375\n",
      "2017-09-23T16:15:13.451033: step 45, graph loss 147.07\n",
      "2017-09-23T16:15:15.092879: step 46, label loss 0.0693766, p1 0.203125, p3 0.130208, p5 0.1\n",
      "2017-09-23T16:15:15.150493: step 46, graph loss 135.797\n",
      "2017-09-23T16:15:16.794698: step 47, label loss 0.0659857, p1 0.078125, p3 0.09375, p5 0.0875\n",
      "2017-09-23T16:15:16.843962: step 47, graph loss 144.66\n",
      "2017-09-23T16:15:18.497667: step 48, label loss 0.0612618, p1 0.078125, p3 0.078125, p5 0.078125\n",
      "2017-09-23T16:15:18.548073: step 48, graph loss 146.109\n",
      "2017-09-23T16:15:20.204157: step 49, label loss 0.0668414, p1 0.140625, p3 0.109375, p5 0.109375\n",
      "2017-09-23T16:15:20.253999: step 49, graph loss 139.249\n",
      "2017-09-23T16:15:21.916215: step 50, label loss 0.064152, p1 0.140625, p3 0.119792, p5 0.1\n",
      "2017-09-23T16:15:21.964270: step 50, graph loss 125.837\n",
      "2017-09-23T16:15:23.591756: step 51, label loss 0.0673953, p1 0.1875, p3 0.109375, p5 0.10625\n",
      "2017-09-23T16:15:23.645509: step 51, graph loss 118.075\n",
      "2017-09-23T16:15:25.306079: step 52, label loss 0.0625118, p1 0.078125, p3 0.0989583, p5 0.0875\n",
      "2017-09-23T16:15:25.356010: step 52, graph loss 126.767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-23T16:15:26.994647: step 53, label loss 0.0640461, p1 0.125, p3 0.0989583, p5 0.096875\n",
      "2017-09-23T16:15:27.039386: step 53, graph loss 119.315\n",
      "2017-09-23T16:15:28.663191: step 54, label loss 0.0644611, p1 0.171875, p3 0.125, p5 0.10625\n",
      "2017-09-23T16:15:28.711150: step 54, graph loss 141.159\n",
      "2017-09-23T16:15:30.356055: step 55, label loss 0.0602297, p1 0.1875, p3 0.130208, p5 0.103125\n",
      "2017-09-23T16:15:30.401563: step 55, graph loss 125.379\n",
      "2017-09-23T16:15:32.043772: step 56, label loss 0.0638242, p1 0.21875, p3 0.130208, p5 0.103125\n",
      "2017-09-23T16:15:32.091411: step 56, graph loss 145.913\n",
      "2017-09-23T16:15:33.763648: step 57, label loss 0.0599512, p1 0.171875, p3 0.130208, p5 0.10625\n",
      "2017-09-23T16:15:33.811067: step 57, graph loss 134.918\n",
      "2017-09-23T16:15:35.472328: step 58, label loss 0.0650965, p1 0.21875, p3 0.135417, p5 0.109375\n",
      "2017-09-23T16:15:35.519970: step 58, graph loss 141.108\n",
      "2017-09-23T16:15:37.146324: step 59, label loss 0.0603348, p1 0.25, p3 0.15625, p5 0.11875\n",
      "2017-09-23T16:15:37.193674: step 59, graph loss 131.668\n",
      "2017-09-23T16:15:38.827278: step 60, label loss 0.0654092, p1 0.140625, p3 0.09375, p5 0.084375\n",
      "2017-09-23T16:15:38.877194: step 60, graph loss 115.498\n",
      "2017-09-23T16:15:40.507939: step 61, label loss 0.0657299, p1 0.15625, p3 0.114583, p5 0.084375\n",
      "2017-09-23T16:15:40.559789: step 61, graph loss 154.076\n",
      "2017-09-23T16:15:42.200316: step 62, label loss 0.0590638, p1 0.140625, p3 0.109375, p5 0.09375\n",
      "2017-09-23T16:15:42.245375: step 62, graph loss 129.208\n",
      "2017-09-23T16:15:43.891771: step 63, label loss 0.0607564, p1 0.1875, p3 0.114583, p5 0.08125\n",
      "2017-09-23T16:15:43.973124: step 63, graph loss 105.052\n",
      "2017-09-23T16:15:45.662769: step 64, label loss 0.0558891, p1 0.203125, p3 0.140625, p5 0.10625\n",
      "2017-09-23T16:15:45.710717: step 64, graph loss 143.681\n",
      "2017-09-23T16:15:47.340541: step 65, label loss 0.0657377, p1 0.171875, p3 0.125, p5 0.121875\n",
      "2017-09-23T16:15:47.387389: step 65, graph loss 135.851\n",
      "2017-09-23T16:15:49.027468: step 66, label loss 0.0595069, p1 0.109375, p3 0.0677083, p5 0.084375\n",
      "2017-09-23T16:15:49.077537: step 66, graph loss 109.96\n",
      "2017-09-23T16:15:50.710420: step 67, label loss 0.0534246, p1 0.1875, p3 0.135417, p5 0.11875\n",
      "2017-09-23T16:15:50.760119: step 67, graph loss 151.604\n",
      "2017-09-23T16:15:52.408859: step 68, label loss 0.0610239, p1 0.1875, p3 0.161458, p5 0.146875\n",
      "2017-09-23T16:15:52.461262: step 68, graph loss 140.788\n",
      "2017-09-23T16:15:54.098667: step 69, label loss 0.0659747, p1 0.046875, p3 0.046875, p5 0.0625\n",
      "2017-09-23T16:15:54.173045: step 69, graph loss 135.801\n",
      "2017-09-23T16:15:55.931581: step 70, label loss 0.0591102, p1 0.15625, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:15:55.984094: step 70, graph loss 119.654\n",
      "2017-09-23T16:15:57.664140: step 71, label loss 0.0659107, p1 0.15625, p3 0.09375, p5 0.071875\n",
      "2017-09-23T16:15:57.719080: step 71, graph loss 136.76\n",
      "2017-09-23T16:15:58.365351: step 72, label loss 0.0743307, p1 0.136364, p3 0.106061, p5 0.0818182\n",
      "2017-09-23T16:15:58.416784: step 72, graph loss 116.074\n",
      "2017-09-23T16:16:00.025554: step 73, label loss 0.057348, p1 0.109375, p3 0.104167, p5 0.096875\n",
      "2017-09-23T16:16:00.079146: step 73, graph loss 107.579\n",
      "2017-09-23T16:16:01.709687: step 74, label loss 0.0547106, p1 0.296875, p3 0.177083, p5 0.14375\n",
      "2017-09-23T16:16:01.765656: step 74, graph loss 144.227\n",
      "2017-09-23T16:16:03.366501: step 75, label loss 0.0545326, p1 0.125, p3 0.166667, p5 0.128125\n",
      "2017-09-23T16:16:03.419743: step 75, graph loss 117.745\n",
      "2017-09-23T16:16:05.069895: step 76, label loss 0.0570899, p1 0.171875, p3 0.109375, p5 0.084375\n",
      "2017-09-23T16:16:05.121007: step 76, graph loss 128.855\n",
      "2017-09-23T16:16:06.735773: step 77, label loss 0.0617932, p1 0.1875, p3 0.104167, p5 0.08125\n",
      "2017-09-23T16:16:06.789794: step 77, graph loss 108.272\n",
      "2017-09-23T16:16:08.392777: step 78, label loss 0.0605948, p1 0.21875, p3 0.15625, p5 0.11875\n",
      "2017-09-23T16:16:08.441646: step 78, graph loss 110.246\n",
      "2017-09-23T16:16:10.093240: step 79, label loss 0.0604916, p1 0.1875, p3 0.114583, p5 0.090625\n",
      "2017-09-23T16:16:10.141709: step 79, graph loss 131.228\n",
      "2017-09-23T16:16:11.774532: step 80, label loss 0.0590285, p1 0.21875, p3 0.114583, p5 0.10625\n",
      "2017-09-23T16:16:11.825104: step 80, graph loss 121.543\n",
      "2017-09-23T16:16:13.483494: step 81, label loss 0.0569865, p1 0.203125, p3 0.119792, p5 0.1\n",
      "2017-09-23T16:16:13.534597: step 81, graph loss 122.357\n",
      "2017-09-23T16:16:15.197408: step 82, label loss 0.0544146, p1 0.1875, p3 0.130208, p5 0.09375\n",
      "2017-09-23T16:16:15.250730: step 82, graph loss 134.513\n",
      "2017-09-23T16:16:16.959967: step 83, label loss 0.0578854, p1 0.171875, p3 0.109375, p5 0.0875\n",
      "2017-09-23T16:16:17.017068: step 83, graph loss 109.402\n",
      "2017-09-23T16:16:18.661807: step 84, label loss 0.0593727, p1 0.125, p3 0.114583, p5 0.1\n",
      "2017-09-23T16:16:18.716149: step 84, graph loss 156.062\n",
      "2017-09-23T16:16:20.372781: step 85, label loss 0.0555385, p1 0.078125, p3 0.104167, p5 0.1125\n",
      "2017-09-23T16:16:20.428431: step 85, graph loss 112.548\n",
      "2017-09-23T16:16:22.137874: step 86, label loss 0.0617977, p1 0.09375, p3 0.0729167, p5 0.071875\n",
      "2017-09-23T16:16:22.190344: step 86, graph loss 123.84\n",
      "2017-09-23T16:16:23.835125: step 87, label loss 0.0518487, p1 0.1875, p3 0.130208, p5 0.121875\n",
      "2017-09-23T16:16:23.891740: step 87, graph loss 118.176\n",
      "2017-09-23T16:16:25.532569: step 88, label loss 0.0522947, p1 0.109375, p3 0.09375, p5 0.084375\n",
      "2017-09-23T16:16:25.590475: step 88, graph loss 85.0239\n",
      "2017-09-23T16:16:27.282292: step 89, label loss 0.0598111, p1 0.09375, p3 0.104167, p5 0.084375\n",
      "2017-09-23T16:16:27.331966: step 89, graph loss 139.983\n",
      "2017-09-23T16:16:29.006344: step 90, label loss 0.0573294, p1 0.15625, p3 0.140625, p5 0.109375\n",
      "2017-09-23T16:16:29.058449: step 90, graph loss 134.303\n",
      "2017-09-23T16:16:30.701832: step 91, label loss 0.0599896, p1 0.15625, p3 0.0989583, p5 0.09375\n",
      "2017-09-23T16:16:30.753829: step 91, graph loss 113.427\n",
      "2017-09-23T16:16:32.410551: step 92, label loss 0.0517938, p1 0.109375, p3 0.0989583, p5 0.1\n",
      "2017-09-23T16:16:32.460490: step 92, graph loss 125.735\n",
      "2017-09-23T16:16:34.091534: step 93, label loss 0.0555832, p1 0.140625, p3 0.145833, p5 0.109375\n",
      "2017-09-23T16:16:34.145082: step 93, graph loss 120.485\n",
      "2017-09-23T16:16:35.766626: step 94, label loss 0.054276, p1 0.140625, p3 0.125, p5 0.096875\n",
      "2017-09-23T16:16:35.820276: step 94, graph loss 116.749\n",
      "2017-09-23T16:16:37.472444: step 95, label loss 0.0544239, p1 0.140625, p3 0.130208, p5 0.109375\n",
      "2017-09-23T16:16:37.527809: step 95, graph loss 108.24\n",
      "2017-09-23T16:16:39.171773: step 96, label loss 0.0543211, p1 0.171875, p3 0.109375, p5 0.08125\n",
      "2017-09-23T16:16:39.234866: step 96, graph loss 124.15\n",
      "2017-09-23T16:16:40.887517: step 97, label loss 0.0510971, p1 0.109375, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:16:40.941969: step 97, graph loss 97.932\n",
      "2017-09-23T16:16:42.598499: step 98, label loss 0.0530022, p1 0.15625, p3 0.104167, p5 0.103125\n",
      "2017-09-23T16:16:42.649658: step 98, graph loss 121.074\n",
      "2017-09-23T16:16:44.301546: step 99, label loss 0.0541308, p1 0.15625, p3 0.109375, p5 0.090625\n",
      "2017-09-23T16:16:44.353707: step 99, graph loss 149.975\n",
      "2017-09-23T16:16:45.996762: step 100, label loss 0.0576282, p1 0.1875, p3 0.104167, p5 0.10625\n",
      "2017-09-23T16:16:46.050708: step 100, graph loss 124.738\n",
      "\n",
      "Evaluation:\n",
      "2017-09-23T16:16:50.102760: step 100, label loss 0.0485351, p1 0.287379, p3 0.18123, p5 0.146408\n",
      "\n",
      "2017-09-23T16:16:51.780553: step 101, label loss 0.0550152, p1 0.1875, p3 0.130208, p5 0.115625\n",
      "2017-09-23T16:16:51.833907: step 101, graph loss 128.042\n",
      "2017-09-23T16:16:53.452121: step 102, label loss 0.0551063, p1 0.109375, p3 0.0989583, p5 0.10625\n",
      "2017-09-23T16:16:53.506818: step 102, graph loss 112.138\n",
      "2017-09-23T16:16:55.149455: step 103, label loss 0.0571271, p1 0.125, p3 0.130208, p5 0.096875\n",
      "2017-09-23T16:16:55.206651: step 103, graph loss 118.649\n",
      "2017-09-23T16:16:56.865897: step 104, label loss 0.052025, p1 0.140625, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:16:56.921660: step 104, graph loss 135.037\n",
      "2017-09-23T16:16:58.637131: step 105, label loss 0.053169, p1 0.1875, p3 0.114583, p5 0.090625\n",
      "2017-09-23T16:16:58.693450: step 105, graph loss 123.416\n",
      "2017-09-23T16:17:00.315538: step 106, label loss 0.0526077, p1 0.109375, p3 0.104167, p5 0.1\n",
      "2017-09-23T16:17:00.370179: step 106, graph loss 98.7311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-23T16:17:02.104260: step 107, label loss 0.0532578, p1 0.140625, p3 0.109375, p5 0.10625\n",
      "2017-09-23T16:17:02.159429: step 107, graph loss 106.569\n",
      "2017-09-23T16:17:03.818466: step 108, label loss 0.0582029, p1 0.0625, p3 0.0989583, p5 0.09375\n",
      "2017-09-23T16:17:03.878189: step 108, graph loss 109.457\n",
      "2017-09-23T16:17:05.506372: step 109, label loss 0.0529901, p1 0.171875, p3 0.109375, p5 0.096875\n",
      "2017-09-23T16:17:05.561482: step 109, graph loss 95.3499\n",
      "2017-09-23T16:17:07.244152: step 110, label loss 0.050586, p1 0.09375, p3 0.0729167, p5 0.08125\n",
      "2017-09-23T16:17:07.298398: step 110, graph loss 89.3025\n",
      "2017-09-23T16:17:09.017414: step 111, label loss 0.0546352, p1 0.15625, p3 0.140625, p5 0.1125\n",
      "2017-09-23T16:17:09.073174: step 111, graph loss 120.993\n",
      "2017-09-23T16:17:10.764453: step 112, label loss 0.0553882, p1 0.109375, p3 0.0885417, p5 0.084375\n",
      "2017-09-23T16:17:10.818508: step 112, graph loss 129.256\n",
      "2017-09-23T16:17:12.493678: step 113, label loss 0.0537053, p1 0.171875, p3 0.114583, p5 0.1\n",
      "2017-09-23T16:17:12.544769: step 113, graph loss 90.626\n",
      "2017-09-23T16:17:14.137097: step 114, label loss 0.0589009, p1 0.203125, p3 0.130208, p5 0.103125\n",
      "2017-09-23T16:17:14.191204: step 114, graph loss 101.525\n",
      "2017-09-23T16:17:15.807886: step 115, label loss 0.0595084, p1 0.109375, p3 0.0885417, p5 0.084375\n",
      "2017-09-23T16:17:15.861491: step 115, graph loss 117.929\n",
      "2017-09-23T16:17:17.499869: step 116, label loss 0.0474366, p1 0.234375, p3 0.145833, p5 0.10625\n",
      "2017-09-23T16:17:17.558212: step 116, graph loss 119.809\n",
      "2017-09-23T16:17:19.171139: step 117, label loss 0.0505666, p1 0.203125, p3 0.09375, p5 0.075\n",
      "2017-09-23T16:17:19.237324: step 117, graph loss 101.093\n",
      "2017-09-23T16:17:20.850847: step 118, label loss 0.0614354, p1 0.125, p3 0.145833, p5 0.1125\n",
      "2017-09-23T16:17:20.909502: step 118, graph loss 128.298\n",
      "2017-09-23T16:17:22.533090: step 119, label loss 0.0562646, p1 0.109375, p3 0.0833333, p5 0.0875\n",
      "2017-09-23T16:17:22.597054: step 119, graph loss 103.228\n",
      "2017-09-23T16:17:24.224979: step 120, label loss 0.0559237, p1 0.15625, p3 0.119792, p5 0.096875\n",
      "2017-09-23T16:17:24.287204: step 120, graph loss 132.823\n",
      "2017-09-23T16:17:25.926284: step 121, label loss 0.0521113, p1 0.1875, p3 0.119792, p5 0.09375\n",
      "2017-09-23T16:17:25.984037: step 121, graph loss 127.918\n",
      "2017-09-23T16:17:27.656582: step 122, label loss 0.0561715, p1 0.140625, p3 0.104167, p5 0.084375\n",
      "2017-09-23T16:17:27.712495: step 122, graph loss 112.006\n",
      "2017-09-23T16:17:29.325598: step 123, label loss 0.0552029, p1 0.15625, p3 0.130208, p5 0.1125\n",
      "2017-09-23T16:17:29.383745: step 123, graph loss 108.187\n",
      "2017-09-23T16:17:31.069800: step 124, label loss 0.0501438, p1 0.109375, p3 0.151042, p5 0.121875\n",
      "2017-09-23T16:17:31.125874: step 124, graph loss 89.6309\n",
      "2017-09-23T16:17:32.821792: step 125, label loss 0.0506904, p1 0.15625, p3 0.114583, p5 0.084375\n",
      "2017-09-23T16:17:32.878244: step 125, graph loss 94.6794\n",
      "2017-09-23T16:17:34.509248: step 126, label loss 0.0503077, p1 0.171875, p3 0.140625, p5 0.109375\n",
      "2017-09-23T16:17:34.563058: step 126, graph loss 100.803\n",
      "2017-09-23T16:17:36.180472: step 127, label loss 0.0527152, p1 0.171875, p3 0.119792, p5 0.1\n",
      "2017-09-23T16:17:36.233641: step 127, graph loss 86.1591\n",
      "2017-09-23T16:17:37.853499: step 128, label loss 0.05794, p1 0.125, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:17:37.908809: step 128, graph loss 75.8434\n",
      "2017-09-23T16:17:39.519614: step 129, label loss 0.0617304, p1 0.09375, p3 0.0885417, p5 0.078125\n",
      "2017-09-23T16:17:39.587424: step 129, graph loss 114.738\n",
      "2017-09-23T16:17:41.199473: step 130, label loss 0.0520806, p1 0.078125, p3 0.0885417, p5 0.084375\n",
      "2017-09-23T16:17:41.262552: step 130, graph loss 99.3589\n",
      "2017-09-23T16:17:42.908483: step 131, label loss 0.0517928, p1 0.171875, p3 0.104167, p5 0.09375\n",
      "2017-09-23T16:17:42.968283: step 131, graph loss 113.42\n",
      "2017-09-23T16:17:44.599959: step 132, label loss 0.0545341, p1 0.25, p3 0.161458, p5 0.115625\n",
      "2017-09-23T16:17:44.662283: step 132, graph loss 94.5525\n",
      "2017-09-23T16:17:46.299268: step 133, label loss 0.0570825, p1 0.203125, p3 0.15625, p5 0.128125\n",
      "2017-09-23T16:17:46.355293: step 133, graph loss 106.219\n",
      "2017-09-23T16:17:48.028240: step 134, label loss 0.0611376, p1 0.078125, p3 0.104167, p5 0.078125\n",
      "2017-09-23T16:17:48.085501: step 134, graph loss 100.861\n",
      "2017-09-23T16:17:49.756805: step 135, label loss 0.0507392, p1 0.140625, p3 0.109375, p5 0.1\n",
      "2017-09-23T16:17:49.839345: step 135, graph loss 137.702\n",
      "2017-09-23T16:17:51.515770: step 136, label loss 0.0483948, p1 0.109375, p3 0.104167, p5 0.084375\n",
      "2017-09-23T16:17:51.577967: step 136, graph loss 115.41\n",
      "2017-09-23T16:17:53.217497: step 137, label loss 0.0536245, p1 0.171875, p3 0.145833, p5 0.115625\n",
      "2017-09-23T16:17:53.272369: step 137, graph loss 121.339\n",
      "2017-09-23T16:17:54.909888: step 138, label loss 0.0519772, p1 0.078125, p3 0.0989583, p5 0.1125\n",
      "2017-09-23T16:17:54.967003: step 138, graph loss 101.267\n",
      "2017-09-23T16:17:56.600767: step 139, label loss 0.050804, p1 0.140625, p3 0.119792, p5 0.10625\n",
      "2017-09-23T16:17:56.660145: step 139, graph loss 87.6787\n",
      "2017-09-23T16:17:58.376577: step 140, label loss 0.0510651, p1 0.1875, p3 0.119792, p5 0.115625\n",
      "2017-09-23T16:17:58.440254: step 140, graph loss 119.302\n",
      "2017-09-23T16:18:00.096509: step 141, label loss 0.0460123, p1 0.203125, p3 0.161458, p5 0.140625\n",
      "2017-09-23T16:18:00.162653: step 141, graph loss 115.937\n",
      "2017-09-23T16:18:01.854057: step 142, label loss 0.0506208, p1 0.171875, p3 0.09375, p5 0.090625\n",
      "2017-09-23T16:18:01.919562: step 142, graph loss 123.31\n",
      "2017-09-23T16:18:03.550491: step 143, label loss 0.0522992, p1 0.125, p3 0.104167, p5 0.0875\n",
      "2017-09-23T16:18:03.610941: step 143, graph loss 97.4782\n",
      "2017-09-23T16:18:05.325651: step 144, label loss 0.0513821, p1 0.078125, p3 0.0885417, p5 0.090625\n",
      "2017-09-23T16:18:05.383134: step 144, graph loss 80.0525\n",
      "2017-09-23T16:18:06.011624: step 145, label loss 0.0592432, p1 0.136364, p3 0.0606061, p5 0.0545455\n",
      "2017-09-23T16:18:06.069998: step 145, graph loss 101.659\n",
      "2017-09-23T16:18:07.754017: step 146, label loss 0.0531191, p1 0.28125, p3 0.166667, p5 0.134375\n",
      "2017-09-23T16:18:07.825867: step 146, graph loss 109.815\n",
      "2017-09-23T16:18:09.465989: step 147, label loss 0.0500697, p1 0.1875, p3 0.130208, p5 0.103125\n",
      "2017-09-23T16:18:09.526357: step 147, graph loss 112.936\n",
      "2017-09-23T16:18:11.174757: step 148, label loss 0.0515485, p1 0.203125, p3 0.145833, p5 0.109375\n",
      "2017-09-23T16:18:11.232687: step 148, graph loss 89.519\n",
      "2017-09-23T16:18:12.893236: step 149, label loss 0.0580274, p1 0.171875, p3 0.125, p5 0.096875\n",
      "2017-09-23T16:18:12.954263: step 149, graph loss 88.4192\n",
      "2017-09-23T16:18:14.635735: step 150, label loss 0.0500106, p1 0.140625, p3 0.119792, p5 0.096875\n",
      "2017-09-23T16:18:14.699304: step 150, graph loss 114.414\n",
      "2017-09-23T16:18:16.381630: step 151, label loss 0.0468522, p1 0.15625, p3 0.114583, p5 0.1\n",
      "2017-09-23T16:18:16.444963: step 151, graph loss 106.065\n",
      "2017-09-23T16:18:18.077545: step 152, label loss 0.0528927, p1 0.1875, p3 0.145833, p5 0.10625\n",
      "2017-09-23T16:18:18.138119: step 152, graph loss 107.505\n",
      "2017-09-23T16:18:19.797006: step 153, label loss 0.0511264, p1 0.21875, p3 0.125, p5 0.09375\n",
      "2017-09-23T16:18:19.864347: step 153, graph loss 135.992\n",
      "2017-09-23T16:18:21.606164: step 154, label loss 0.0462285, p1 0.140625, p3 0.0989583, p5 0.071875\n",
      "2017-09-23T16:18:21.669132: step 154, graph loss 105.707\n",
      "2017-09-23T16:18:23.300888: step 155, label loss 0.0493798, p1 0.234375, p3 0.145833, p5 0.125\n",
      "2017-09-23T16:18:23.362296: step 155, graph loss 107.114\n",
      "2017-09-23T16:18:25.017258: step 156, label loss 0.0466049, p1 0.25, p3 0.125, p5 0.1\n",
      "2017-09-23T16:18:25.079738: step 156, graph loss 111.088\n",
      "2017-09-23T16:18:26.744870: step 157, label loss 0.0485168, p1 0.15625, p3 0.135417, p5 0.090625\n",
      "2017-09-23T16:18:26.802493: step 157, graph loss 94.2436\n",
      "2017-09-23T16:18:28.437867: step 158, label loss 0.0548979, p1 0.1875, p3 0.161458, p5 0.125\n",
      "2017-09-23T16:18:28.496741: step 158, graph loss 97.9957\n",
      "2017-09-23T16:18:30.152980: step 159, label loss 0.0498796, p1 0.0625, p3 0.0833333, p5 0.08125\n",
      "2017-09-23T16:18:30.213376: step 159, graph loss 110.305\n",
      "2017-09-23T16:18:31.864655: step 160, label loss 0.0442496, p1 0.265625, p3 0.151042, p5 0.125\n",
      "2017-09-23T16:18:31.921818: step 160, graph loss 94.5336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-23T16:18:33.560709: step 161, label loss 0.0527037, p1 0.125, p3 0.0989583, p5 0.096875\n",
      "2017-09-23T16:18:33.620712: step 161, graph loss 99.178\n",
      "2017-09-23T16:18:35.240543: step 162, label loss 0.0473902, p1 0.140625, p3 0.0989583, p5 0.1\n",
      "2017-09-23T16:18:35.305962: step 162, graph loss 104.828\n",
      "2017-09-23T16:18:36.936738: step 163, label loss 0.0502322, p1 0.21875, p3 0.130208, p5 0.11875\n",
      "2017-09-23T16:18:37.000001: step 163, graph loss 85.8571\n",
      "2017-09-23T16:18:38.620950: step 164, label loss 0.0524488, p1 0.15625, p3 0.145833, p5 0.14375\n",
      "2017-09-23T16:18:38.683015: step 164, graph loss 99.4061\n",
      "2017-09-23T16:18:40.309425: step 165, label loss 0.0516906, p1 0.140625, p3 0.135417, p5 0.134375\n",
      "2017-09-23T16:18:40.369779: step 165, graph loss 105.829\n",
      "2017-09-23T16:18:41.992866: step 166, label loss 0.0503789, p1 0.0625, p3 0.078125, p5 0.0875\n",
      "2017-09-23T16:18:42.057130: step 166, graph loss 102.402\n",
      "2017-09-23T16:18:43.685766: step 167, label loss 0.0524008, p1 0.1875, p3 0.114583, p5 0.09375\n",
      "2017-09-23T16:18:43.743681: step 167, graph loss 84.2193\n",
      "2017-09-23T16:18:45.368578: step 168, label loss 0.0455918, p1 0.109375, p3 0.119792, p5 0.109375\n",
      "2017-09-23T16:18:45.430412: step 168, graph loss 101.573\n",
      "2017-09-23T16:18:47.091867: step 169, label loss 0.0511369, p1 0.109375, p3 0.161458, p5 0.134375\n",
      "2017-09-23T16:18:47.155482: step 169, graph loss 120.741\n",
      "2017-09-23T16:18:48.797772: step 170, label loss 0.0528569, p1 0.140625, p3 0.171875, p5 0.121875\n",
      "2017-09-23T16:18:48.858643: step 170, graph loss 111.69\n",
      "2017-09-23T16:18:50.518459: step 171, label loss 0.0517491, p1 0.140625, p3 0.135417, p5 0.109375\n",
      "2017-09-23T16:18:50.580838: step 171, graph loss 95.2101\n",
      "2017-09-23T16:18:52.228549: step 172, label loss 0.0511098, p1 0.140625, p3 0.151042, p5 0.121875\n",
      "2017-09-23T16:18:52.288233: step 172, graph loss 102.238\n",
      "2017-09-23T16:18:53.926088: step 173, label loss 0.0487915, p1 0.140625, p3 0.104167, p5 0.10625\n",
      "2017-09-23T16:18:53.986747: step 173, graph loss 102.733\n",
      "2017-09-23T16:18:55.656248: step 174, label loss 0.0418287, p1 0.234375, p3 0.197917, p5 0.146875\n",
      "2017-09-23T16:18:55.721644: step 174, graph loss 94.0727\n",
      "2017-09-23T16:18:57.373890: step 175, label loss 0.0515032, p1 0.21875, p3 0.151042, p5 0.13125\n",
      "2017-09-23T16:18:57.437637: step 175, graph loss 76.739\n",
      "2017-09-23T16:18:59.107585: step 176, label loss 0.0532916, p1 0.140625, p3 0.151042, p5 0.121875\n",
      "2017-09-23T16:18:59.172566: step 176, graph loss 128.065\n",
      "2017-09-23T16:19:00.914210: step 177, label loss 0.0511542, p1 0.15625, p3 0.161458, p5 0.125\n",
      "2017-09-23T16:19:00.980315: step 177, graph loss 120.296\n",
      "2017-09-23T16:19:02.652319: step 178, label loss 0.0516638, p1 0.21875, p3 0.145833, p5 0.115625\n",
      "2017-09-23T16:19:02.718404: step 178, graph loss 115.706\n",
      "2017-09-23T16:19:04.377927: step 179, label loss 0.051433, p1 0.21875, p3 0.161458, p5 0.1375\n",
      "2017-09-23T16:19:04.443126: step 179, graph loss 86.4617\n",
      "2017-09-23T16:19:06.113236: step 180, label loss 0.0530001, p1 0.25, p3 0.151042, p5 0.134375\n",
      "2017-09-23T16:19:06.176305: step 180, graph loss 104.65\n",
      "2017-09-23T16:19:07.818039: step 181, label loss 0.0442074, p1 0.25, p3 0.140625, p5 0.115625\n",
      "2017-09-23T16:19:07.879604: step 181, graph loss 65.3092\n",
      "2017-09-23T16:19:09.525992: step 182, label loss 0.0529608, p1 0.046875, p3 0.078125, p5 0.08125\n",
      "2017-09-23T16:19:09.586646: step 182, graph loss 81.2125\n",
      "2017-09-23T16:19:11.234134: step 183, label loss 0.0484996, p1 0.1875, p3 0.140625, p5 0.115625\n",
      "2017-09-23T16:19:11.296001: step 183, graph loss 72.7751\n",
      "2017-09-23T16:19:12.967651: step 184, label loss 0.0549657, p1 0.109375, p3 0.109375, p5 0.090625\n",
      "2017-09-23T16:19:13.029956: step 184, graph loss 75.5464\n",
      "2017-09-23T16:19:14.664161: step 185, label loss 0.0423537, p1 0.171875, p3 0.161458, p5 0.134375\n",
      "2017-09-23T16:19:14.724832: step 185, graph loss 122.097\n",
      "2017-09-23T16:19:16.371322: step 186, label loss 0.046237, p1 0.15625, p3 0.130208, p5 0.0875\n",
      "2017-09-23T16:19:16.439686: step 186, graph loss 85.0351\n",
      "2017-09-23T16:19:18.059901: step 187, label loss 0.05105, p1 0.21875, p3 0.151042, p5 0.115625\n",
      "2017-09-23T16:19:18.123301: step 187, graph loss 120.693\n",
      "2017-09-23T16:19:19.775918: step 188, label loss 0.0500938, p1 0.140625, p3 0.125, p5 0.125\n",
      "2017-09-23T16:19:19.838571: step 188, graph loss 107.793\n",
      "2017-09-23T16:19:21.477054: step 189, label loss 0.0494628, p1 0.25, p3 0.15625, p5 0.13125\n",
      "2017-09-23T16:19:21.544204: step 189, graph loss 81.719\n",
      "2017-09-23T16:19:23.194667: step 190, label loss 0.0498823, p1 0.125, p3 0.114583, p5 0.11875\n",
      "2017-09-23T16:19:23.258347: step 190, graph loss 112.893\n",
      "2017-09-23T16:19:24.972840: step 191, label loss 0.0527374, p1 0.15625, p3 0.145833, p5 0.103125\n",
      "2017-09-23T16:19:25.040702: step 191, graph loss 91.4559\n",
      "2017-09-23T16:19:26.760252: step 192, label loss 0.0481599, p1 0.109375, p3 0.119792, p5 0.10625\n",
      "2017-09-23T16:19:26.828490: step 192, graph loss 109.314\n",
      "2017-09-23T16:19:28.514543: step 193, label loss 0.0498539, p1 0.109375, p3 0.130208, p5 0.109375\n",
      "2017-09-23T16:19:28.582675: step 193, graph loss 75.5979\n",
      "2017-09-23T16:19:30.209941: step 194, label loss 0.0465793, p1 0.125, p3 0.125, p5 0.103125\n",
      "2017-09-23T16:19:30.276947: step 194, graph loss 83.7946\n",
      "2017-09-23T16:19:31.927817: step 195, label loss 0.0476577, p1 0.140625, p3 0.130208, p5 0.10625\n",
      "2017-09-23T16:19:31.989208: step 195, graph loss 139.564\n",
      "2017-09-23T16:19:33.609772: step 196, label loss 0.0493708, p1 0.25, p3 0.161458, p5 0.128125\n",
      "2017-09-23T16:19:33.674588: step 196, graph loss 79.8555\n",
      "2017-09-23T16:19:35.287620: step 197, label loss 0.0462735, p1 0.21875, p3 0.171875, p5 0.128125\n",
      "2017-09-23T16:19:35.350717: step 197, graph loss 85.526\n",
      "2017-09-23T16:19:36.999143: step 198, label loss 0.0470352, p1 0.046875, p3 0.0885417, p5 0.071875\n",
      "2017-09-23T16:19:37.062272: step 198, graph loss 116.294\n",
      "2017-09-23T16:19:38.712827: step 199, label loss 0.0466028, p1 0.203125, p3 0.0989583, p5 0.084375\n",
      "2017-09-23T16:19:38.777384: step 199, graph loss 110.222\n",
      "2017-09-23T16:19:40.434615: step 200, label loss 0.0457641, p1 0.25, p3 0.192708, p5 0.15625\n",
      "2017-09-23T16:19:40.501171: step 200, graph loss 82.3547\n",
      "\n",
      "Evaluation:\n",
      "2017-09-23T16:19:44.391274: step 200, label loss 0.0435622, p1 0.287379, p3 0.19288, p5 0.149126\n",
      "\n",
      "2017-09-23T16:19:46.079724: step 201, label loss 0.0466432, p1 0.203125, p3 0.130208, p5 0.109375\n",
      "2017-09-23T16:19:46.147358: step 201, graph loss 91.9822\n",
      "2017-09-23T16:19:47.759214: step 202, label loss 0.0512229, p1 0.171875, p3 0.151042, p5 0.115625\n",
      "2017-09-23T16:19:47.821931: step 202, graph loss 74.4298\n",
      "2017-09-23T16:19:49.460895: step 203, label loss 0.0424733, p1 0.140625, p3 0.119792, p5 0.109375\n",
      "2017-09-23T16:19:49.528661: step 203, graph loss 80.2822\n",
      "2017-09-23T16:19:51.145697: step 204, label loss 0.0484988, p1 0.203125, p3 0.161458, p5 0.13125\n",
      "2017-09-23T16:19:51.210290: step 204, graph loss 97.5411\n",
      "2017-09-23T16:19:52.835129: step 205, label loss 0.0433077, p1 0.125, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:19:52.901236: step 205, graph loss 94.1324\n",
      "2017-09-23T16:19:54.553585: step 206, label loss 0.0426946, p1 0.21875, p3 0.161458, p5 0.125\n",
      "2017-09-23T16:19:54.621508: step 206, graph loss 99.485\n",
      "2017-09-23T16:19:56.234524: step 207, label loss 0.0505932, p1 0.125, p3 0.125, p5 0.08125\n",
      "2017-09-23T16:19:56.300705: step 207, graph loss 103.178\n",
      "2017-09-23T16:19:57.908250: step 208, label loss 0.0487946, p1 0.203125, p3 0.140625, p5 0.115625\n",
      "2017-09-23T16:19:57.978156: step 208, graph loss 83.8881\n",
      "2017-09-23T16:19:59.655851: step 209, label loss 0.0482903, p1 0.140625, p3 0.119792, p5 0.090625\n",
      "2017-09-23T16:19:59.729207: step 209, graph loss 121.623\n",
      "2017-09-23T16:20:01.419071: step 210, label loss 0.0508666, p1 0.171875, p3 0.119792, p5 0.10625\n",
      "2017-09-23T16:20:01.485736: step 210, graph loss 84.4214\n",
      "2017-09-23T16:20:03.117916: step 211, label loss 0.0436114, p1 0.109375, p3 0.125, p5 0.1\n",
      "2017-09-23T16:20:03.182875: step 211, graph loss 118.874\n",
      "2017-09-23T16:20:04.868672: step 212, label loss 0.0517574, p1 0.21875, p3 0.140625, p5 0.121875\n",
      "2017-09-23T16:20:04.937963: step 212, graph loss 84.839\n",
      "2017-09-23T16:20:06.581676: step 213, label loss 0.0528692, p1 0.140625, p3 0.135417, p5 0.1\n",
      "2017-09-23T16:20:06.649888: step 213, graph loss 82.842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-23T16:20:08.330878: step 214, label loss 0.0523593, p1 0.15625, p3 0.104167, p5 0.115625\n",
      "2017-09-23T16:20:08.395140: step 214, graph loss 62.746\n",
      "2017-09-23T16:20:10.056365: step 215, label loss 0.0473336, p1 0.15625, p3 0.140625, p5 0.115625\n",
      "2017-09-23T16:20:10.121439: step 215, graph loss 97.2988\n",
      "2017-09-23T16:20:11.769026: step 216, label loss 0.0511587, p1 0.25, p3 0.161458, p5 0.128125\n",
      "2017-09-23T16:20:11.834038: step 216, graph loss 85.1765\n",
      "2017-09-23T16:20:13.459272: step 217, label loss 0.0459071, p1 0.1875, p3 0.140625, p5 0.10625\n",
      "2017-09-23T16:20:13.524506: step 217, graph loss 101.784\n",
      "2017-09-23T16:20:14.146365: step 218, label loss 0.0509102, p1 0.136364, p3 0.0757576, p5 0.0727273\n",
      "2017-09-23T16:20:14.212829: step 218, graph loss 99.8138\n",
      "2017-09-23T16:20:15.873478: step 219, label loss 0.0462685, p1 0.25, p3 0.151042, p5 0.134375\n",
      "2017-09-23T16:20:15.947543: step 219, graph loss 87.0208\n",
      "2017-09-23T16:20:17.655428: step 220, label loss 0.0445792, p1 0.203125, p3 0.140625, p5 0.121875\n",
      "2017-09-23T16:20:17.722740: step 220, graph loss 99.7498\n",
      "2017-09-23T16:20:19.440448: step 221, label loss 0.0421893, p1 0.265625, p3 0.166667, p5 0.134375\n",
      "2017-09-23T16:20:19.510239: step 221, graph loss 103.318\n",
      "2017-09-23T16:20:21.184910: step 222, label loss 0.0442573, p1 0.234375, p3 0.140625, p5 0.121875\n",
      "2017-09-23T16:20:21.255726: step 222, graph loss 98.4998\n",
      "2017-09-23T16:20:22.951944: step 223, label loss 0.0468817, p1 0.203125, p3 0.161458, p5 0.11875\n",
      "2017-09-23T16:20:23.022902: step 223, graph loss 104.054\n",
      "2017-09-23T16:20:24.869633: step 224, label loss 0.0477443, p1 0.28125, p3 0.145833, p5 0.121875\n",
      "2017-09-23T16:20:24.942064: step 224, graph loss 84.6241\n",
      "2017-09-23T16:20:26.586190: step 225, label loss 0.0463921, p1 0.25, p3 0.171875, p5 0.146875\n",
      "2017-09-23T16:20:26.658782: step 225, graph loss 111.457\n",
      "2017-09-23T16:20:28.335957: step 226, label loss 0.0484082, p1 0.21875, p3 0.145833, p5 0.11875\n",
      "2017-09-23T16:20:28.413495: step 226, graph loss 58.8084\n",
      "2017-09-23T16:20:30.122071: step 227, label loss 0.0456062, p1 0.265625, p3 0.166667, p5 0.121875\n",
      "2017-09-23T16:20:30.195162: step 227, graph loss 91.1809\n",
      "2017-09-23T16:20:31.952534: step 228, label loss 0.0445556, p1 0.21875, p3 0.151042, p5 0.10625\n",
      "2017-09-23T16:20:32.029871: step 228, graph loss 115.704\n",
      "2017-09-23T16:20:33.711890: step 229, label loss 0.0500146, p1 0.234375, p3 0.161458, p5 0.10625\n",
      "2017-09-23T16:20:33.779993: step 229, graph loss 95.0572\n",
      "2017-09-23T16:20:35.423153: step 230, label loss 0.0435744, p1 0.328125, p3 0.208333, p5 0.153125\n",
      "2017-09-23T16:20:35.496128: step 230, graph loss 79.1702\n",
      "2017-09-23T16:20:37.168393: step 231, label loss 0.0435241, p1 0.1875, p3 0.140625, p5 0.115625\n",
      "2017-09-23T16:20:37.236099: step 231, graph loss 73.8835\n",
      "2017-09-23T16:20:38.970526: step 232, label loss 0.0452824, p1 0.25, p3 0.171875, p5 0.128125\n",
      "2017-09-23T16:20:39.040389: step 232, graph loss 70.4235\n",
      "2017-09-23T16:20:40.688669: step 233, label loss 0.048495, p1 0.234375, p3 0.140625, p5 0.125\n",
      "2017-09-23T16:20:40.758308: step 233, graph loss 86.3097\n",
      "2017-09-23T16:20:42.420894: step 234, label loss 0.0472066, p1 0.25, p3 0.166667, p5 0.134375\n",
      "2017-09-23T16:20:42.496792: step 234, graph loss 55.3438\n",
      "2017-09-23T16:20:44.162525: step 235, label loss 0.0444207, p1 0.171875, p3 0.166667, p5 0.11875\n",
      "2017-09-23T16:20:44.236583: step 235, graph loss 89.4964\n",
      "2017-09-23T16:20:45.865419: step 236, label loss 0.0420138, p1 0.203125, p3 0.130208, p5 0.125\n",
      "2017-09-23T16:20:45.936044: step 236, graph loss 84.7629\n",
      "2017-09-23T16:20:47.583474: step 237, label loss 0.0447902, p1 0.203125, p3 0.145833, p5 0.115625\n",
      "2017-09-23T16:20:47.655590: step 237, graph loss 99.6884\n",
      "2017-09-23T16:20:49.306621: step 238, label loss 0.0454346, p1 0.265625, p3 0.192708, p5 0.153125\n",
      "2017-09-23T16:20:49.375143: step 238, graph loss 71.6154\n",
      "2017-09-23T16:20:51.047633: step 239, label loss 0.0449133, p1 0.15625, p3 0.114583, p5 0.115625\n",
      "2017-09-23T16:20:51.116961: step 239, graph loss 60.0905\n",
      "2017-09-23T16:20:52.801173: step 240, label loss 0.0536085, p1 0.203125, p3 0.114583, p5 0.10625\n",
      "2017-09-23T16:20:52.874078: step 240, graph loss 70.6618\n",
      "2017-09-23T16:20:54.529633: step 241, label loss 0.0465533, p1 0.1875, p3 0.119792, p5 0.096875\n",
      "2017-09-23T16:20:54.598742: step 241, graph loss 78.2318\n",
      "2017-09-23T16:20:56.240045: step 242, label loss 0.0522739, p1 0.140625, p3 0.0989583, p5 0.090625\n",
      "2017-09-23T16:20:56.307924: step 242, graph loss 74.3475\n",
      "2017-09-23T16:20:57.975018: step 243, label loss 0.044007, p1 0.125, p3 0.15625, p5 0.134375\n",
      "2017-09-23T16:20:58.050682: step 243, graph loss 105.51\n",
      "2017-09-23T16:20:59.676258: step 244, label loss 0.051607, p1 0.203125, p3 0.171875, p5 0.14375\n",
      "2017-09-23T16:20:59.748858: step 244, graph loss 82.1001\n",
      "2017-09-23T16:21:01.417301: step 245, label loss 0.0465538, p1 0.171875, p3 0.114583, p5 0.103125\n",
      "2017-09-23T16:21:01.491567: step 245, graph loss 86.1359\n",
      "2017-09-23T16:21:03.138593: step 246, label loss 0.0475901, p1 0.15625, p3 0.145833, p5 0.115625\n",
      "2017-09-23T16:21:03.213222: step 246, graph loss 101.573\n",
      "2017-09-23T16:21:04.882168: step 247, label loss 0.045711, p1 0.203125, p3 0.171875, p5 0.1375\n",
      "2017-09-23T16:21:05.013035: step 247, graph loss 105.662\n",
      "2017-09-23T16:21:06.730300: step 248, label loss 0.0463579, p1 0.1875, p3 0.145833, p5 0.125\n",
      "2017-09-23T16:21:06.805265: step 248, graph loss 49.9662\n",
      "2017-09-23T16:21:08.468657: step 249, label loss 0.0479356, p1 0.25, p3 0.15625, p5 0.140625\n",
      "2017-09-23T16:21:08.549917: step 249, graph loss 90.8369\n",
      "2017-09-23T16:21:10.214456: step 250, label loss 0.0414096, p1 0.125, p3 0.114583, p5 0.13125\n",
      "2017-09-23T16:21:10.291314: step 250, graph loss 84.2723\n",
      "2017-09-23T16:21:11.939987: step 251, label loss 0.0432245, p1 0.203125, p3 0.15625, p5 0.125\n",
      "2017-09-23T16:21:12.011790: step 251, graph loss 77.087\n",
      "2017-09-23T16:21:13.654133: step 252, label loss 0.0446665, p1 0.1875, p3 0.182292, p5 0.140625\n",
      "2017-09-23T16:21:13.730988: step 252, graph loss 64.4859\n",
      "2017-09-23T16:21:15.394457: step 253, label loss 0.0482906, p1 0.15625, p3 0.140625, p5 0.121875\n",
      "2017-09-23T16:21:15.467731: step 253, graph loss 100.775\n",
      "2017-09-23T16:21:17.153764: step 254, label loss 0.0468995, p1 0.203125, p3 0.171875, p5 0.140625\n",
      "2017-09-23T16:21:17.223996: step 254, graph loss 93.339\n",
      "2017-09-23T16:21:18.929618: step 255, label loss 0.0439626, p1 0.203125, p3 0.15625, p5 0.128125\n",
      "2017-09-23T16:21:19.000228: step 255, graph loss 82.3937\n",
      "2017-09-23T16:21:20.724932: step 256, label loss 0.0443317, p1 0.265625, p3 0.177083, p5 0.13125\n",
      "2017-09-23T16:21:20.796107: step 256, graph loss 71.8439\n",
      "2017-09-23T16:21:22.477421: step 257, label loss 0.0500385, p1 0.234375, p3 0.177083, p5 0.14375\n",
      "2017-09-23T16:21:22.549093: step 257, graph loss 107.361\n",
      "2017-09-23T16:21:24.215597: step 258, label loss 0.0464459, p1 0.25, p3 0.197917, p5 0.1375\n",
      "2017-09-23T16:21:24.285250: step 258, graph loss 68.6374\n",
      "2017-09-23T16:21:25.929343: step 259, label loss 0.0394303, p1 0.234375, p3 0.192708, p5 0.140625\n",
      "2017-09-23T16:21:25.998894: step 259, graph loss 80.1694\n",
      "2017-09-23T16:21:27.711389: step 260, label loss 0.0426727, p1 0.25, p3 0.151042, p5 0.1125\n",
      "2017-09-23T16:21:27.783429: step 260, graph loss 81.1807\n",
      "2017-09-23T16:21:29.461658: step 261, label loss 0.0457119, p1 0.1875, p3 0.140625, p5 0.11875\n",
      "2017-09-23T16:21:29.532000: step 261, graph loss 53.9777\n",
      "2017-09-23T16:21:31.230249: step 262, label loss 0.0454986, p1 0.28125, p3 0.1875, p5 0.14375\n",
      "2017-09-23T16:21:31.304394: step 262, graph loss 97.7892\n",
      "2017-09-23T16:21:32.971829: step 263, label loss 0.0477133, p1 0.203125, p3 0.151042, p5 0.121875\n",
      "2017-09-23T16:21:33.048062: step 263, graph loss 60.8519\n",
      "2017-09-23T16:21:34.702674: step 264, label loss 0.0476728, p1 0.15625, p3 0.109375, p5 0.1\n",
      "2017-09-23T16:21:34.783050: step 264, graph loss 75.9463\n",
      "2017-09-23T16:21:36.408808: step 265, label loss 0.0509932, p1 0.125, p3 0.135417, p5 0.115625\n",
      "2017-09-23T16:21:36.550333: step 265, graph loss 85.3837\n",
      "2017-09-23T16:21:38.232163: step 266, label loss 0.0433005, p1 0.21875, p3 0.1875, p5 0.15\n",
      "2017-09-23T16:21:38.313058: step 266, graph loss 75.1269\n",
      "2017-09-23T16:21:40.014391: step 267, label loss 0.0417684, p1 0.15625, p3 0.114583, p5 0.10625\n",
      "2017-09-23T16:21:40.094253: step 267, graph loss 56.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-23T16:21:41.763807: step 268, label loss 0.0487234, p1 0.1875, p3 0.171875, p5 0.13125\n",
      "2017-09-23T16:21:41.837898: step 268, graph loss 77.1163\n",
      "2017-09-23T16:21:43.462766: step 269, label loss 0.0451714, p1 0.171875, p3 0.109375, p5 0.084375\n",
      "2017-09-23T16:21:43.535756: step 269, graph loss 68.1483\n",
      "2017-09-23T16:21:45.175264: step 270, label loss 0.0468382, p1 0.15625, p3 0.119792, p5 0.103125\n",
      "2017-09-23T16:21:45.247323: step 270, graph loss 95.0827\n",
      "2017-09-23T16:21:46.883942: step 271, label loss 0.0402676, p1 0.28125, p3 0.161458, p5 0.128125\n",
      "2017-09-23T16:21:46.953071: step 271, graph loss 95.2819\n",
      "2017-09-23T16:21:48.581670: step 272, label loss 0.046704, p1 0.265625, p3 0.192708, p5 0.1625\n",
      "2017-09-23T16:21:48.662359: step 272, graph loss 95.0237\n",
      "2017-09-23T16:21:50.303783: step 273, label loss 0.0469143, p1 0.375, p3 0.21875, p5 0.1625\n",
      "2017-09-23T16:21:50.386014: step 273, graph loss 95.8912\n",
      "2017-09-23T16:21:52.005807: step 274, label loss 0.0445276, p1 0.171875, p3 0.15625, p5 0.125\n",
      "2017-09-23T16:21:52.082943: step 274, graph loss 85.1031\n",
      "2017-09-23T16:21:53.720374: step 275, label loss 0.0459568, p1 0.1875, p3 0.140625, p5 0.109375\n",
      "2017-09-23T16:21:53.793092: step 275, graph loss 74.7732\n",
      "2017-09-23T16:21:55.420413: step 276, label loss 0.0477034, p1 0.25, p3 0.145833, p5 0.134375\n",
      "2017-09-23T16:21:55.496168: step 276, graph loss 66.4614\n",
      "2017-09-23T16:21:57.150615: step 277, label loss 0.0421028, p1 0.265625, p3 0.177083, p5 0.125\n",
      "2017-09-23T16:21:57.229925: step 277, graph loss 83.1343\n",
      "2017-09-23T16:21:58.859168: step 278, label loss 0.0461854, p1 0.1875, p3 0.145833, p5 0.1375\n",
      "2017-09-23T16:21:58.940805: step 278, graph loss 92.3702\n",
      "2017-09-23T16:22:00.590320: step 279, label loss 0.0462334, p1 0.171875, p3 0.140625, p5 0.13125\n",
      "2017-09-23T16:22:00.676146: step 279, graph loss 74.4046\n",
      "2017-09-23T16:22:02.331405: step 280, label loss 0.0442361, p1 0.15625, p3 0.140625, p5 0.134375\n",
      "2017-09-23T16:22:02.417068: step 280, graph loss 80.3344\n",
      "2017-09-23T16:22:04.040275: step 281, label loss 0.0463006, p1 0.265625, p3 0.166667, p5 0.140625\n",
      "2017-09-23T16:22:04.118916: step 281, graph loss 65.1749\n",
      "2017-09-23T16:22:05.849937: step 282, label loss 0.0457921, p1 0.21875, p3 0.171875, p5 0.121875\n",
      "2017-09-23T16:22:05.933846: step 282, graph loss 80.3463\n",
      "2017-09-23T16:22:07.575143: step 283, label loss 0.0432007, p1 0.140625, p3 0.0729167, p5 0.084375\n",
      "2017-09-23T16:22:07.648306: step 283, graph loss 61.313\n",
      "2017-09-23T16:22:09.298546: step 284, label loss 0.0446378, p1 0.15625, p3 0.145833, p5 0.11875\n",
      "2017-09-23T16:22:09.373354: step 284, graph loss 67.2082\n",
      "2017-09-23T16:22:11.037235: step 285, label loss 0.0384918, p1 0.109375, p3 0.114583, p5 0.09375\n",
      "2017-09-23T16:22:11.111107: step 285, graph loss 88.0945\n",
      "2017-09-23T16:22:12.753232: step 286, label loss 0.0471521, p1 0.15625, p3 0.135417, p5 0.103125\n",
      "2017-09-23T16:22:12.835121: step 286, graph loss 73.5881\n",
      "2017-09-23T16:22:14.468612: step 287, label loss 0.0458962, p1 0.140625, p3 0.171875, p5 0.1375\n",
      "2017-09-23T16:22:14.542991: step 287, graph loss 81.0847\n",
      "2017-09-23T16:22:16.205563: step 288, label loss 0.0461944, p1 0.21875, p3 0.171875, p5 0.134375\n",
      "2017-09-23T16:22:16.278507: step 288, graph loss 73.7336\n",
      "2017-09-23T16:22:17.946673: step 289, label loss 0.0463063, p1 0.1875, p3 0.161458, p5 0.134375\n",
      "2017-09-23T16:22:18.022459: step 289, graph loss 62.5078\n",
      "2017-09-23T16:22:19.661350: step 290, label loss 0.0465657, p1 0.203125, p3 0.166667, p5 0.115625\n",
      "2017-09-23T16:22:19.737143: step 290, graph loss 57.0568\n",
      "2017-09-23T16:22:20.389509: step 291, label loss 0.038283, p1 0, p3 0.030303, p5 0.0636364\n",
      "2017-09-23T16:22:20.461967: step 291, graph loss 68.6193\n",
      "2017-09-23T16:22:22.121814: step 292, label loss 0.0456605, p1 0.328125, p3 0.203125, p5 0.1625\n",
      "2017-09-23T16:22:22.196465: step 292, graph loss 72.7483\n",
      "2017-09-23T16:22:23.856238: step 293, label loss 0.0452186, p1 0.234375, p3 0.182292, p5 0.128125\n",
      "2017-09-23T16:22:23.931767: step 293, graph loss 65.7117\n",
      "2017-09-23T16:22:25.595942: step 294, label loss 0.041932, p1 0.28125, p3 0.192708, p5 0.134375\n",
      "2017-09-23T16:22:25.670146: step 294, graph loss 66.6462\n",
      "2017-09-23T16:22:27.351474: step 295, label loss 0.0437223, p1 0.1875, p3 0.166667, p5 0.146875\n",
      "2017-09-23T16:22:27.428819: step 295, graph loss 56.3619\n",
      "2017-09-23T16:22:29.102668: step 296, label loss 0.040978, p1 0.203125, p3 0.171875, p5 0.14375\n",
      "2017-09-23T16:22:29.179425: step 296, graph loss 83.2477\n",
      "2017-09-23T16:22:30.852011: step 297, label loss 0.0416148, p1 0.1875, p3 0.145833, p5 0.13125\n",
      "2017-09-23T16:22:30.928755: step 297, graph loss 77.6008\n",
      "2017-09-23T16:22:32.614513: step 298, label loss 0.0448009, p1 0.1875, p3 0.145833, p5 0.11875\n",
      "2017-09-23T16:22:32.696920: step 298, graph loss 62.3135\n",
      "2017-09-23T16:22:34.398477: step 299, label loss 0.0417363, p1 0.234375, p3 0.161458, p5 0.134375\n",
      "2017-09-23T16:22:34.478274: step 299, graph loss 74.9137\n",
      "2017-09-23T16:22:36.144259: step 300, label loss 0.042945, p1 0.21875, p3 0.1875, p5 0.15\n",
      "2017-09-23T16:22:36.223983: step 300, graph loss 61.1162\n",
      "\n",
      "Evaluation:\n",
      "2017-09-23T16:22:40.229304: step 300, label loss 0.040992, p1 0.293204, p3 0.203236, p5 0.16466\n",
      "\n",
      "2017-09-23T16:22:41.922382: step 301, label loss 0.0455715, p1 0.265625, p3 0.177083, p5 0.1375\n",
      "2017-09-23T16:22:41.998553: step 301, graph loss 52.0569\n",
      "2017-09-23T16:22:43.701183: step 302, label loss 0.0434178, p1 0.125, p3 0.171875, p5 0.13125\n",
      "2017-09-23T16:22:43.777056: step 302, graph loss 58.1074\n",
      "2017-09-23T16:22:45.477401: step 303, label loss 0.0474319, p1 0.203125, p3 0.15625, p5 0.121875\n",
      "2017-09-23T16:22:45.551690: step 303, graph loss 83.6787\n",
      "2017-09-23T16:22:47.229126: step 304, label loss 0.0397393, p1 0.21875, p3 0.1875, p5 0.153125\n",
      "2017-09-23T16:22:47.305756: step 304, graph loss 79.9883\n",
      "2017-09-23T16:22:48.969679: step 305, label loss 0.0434686, p1 0.171875, p3 0.161458, p5 0.115625\n",
      "2017-09-23T16:22:49.047300: step 305, graph loss 83.4794\n",
      "2017-09-23T16:22:50.693307: step 306, label loss 0.0402554, p1 0.234375, p3 0.145833, p5 0.128125\n",
      "2017-09-23T16:22:50.770000: step 306, graph loss 85.2274\n",
      "2017-09-23T16:22:52.494685: step 307, label loss 0.0413578, p1 0.140625, p3 0.130208, p5 0.125\n",
      "2017-09-23T16:22:52.572358: step 307, graph loss 66.0842\n",
      "2017-09-23T16:22:54.229637: step 308, label loss 0.0446327, p1 0.171875, p3 0.151042, p5 0.140625\n",
      "2017-09-23T16:22:54.305113: step 308, graph loss 79.4062\n",
      "2017-09-23T16:22:55.957555: step 309, label loss 0.041965, p1 0.265625, p3 0.197917, p5 0.15625\n",
      "2017-09-23T16:22:56.033087: step 309, graph loss 63.1125\n",
      "2017-09-23T16:22:57.714115: step 310, label loss 0.0438439, p1 0.21875, p3 0.145833, p5 0.13125\n",
      "2017-09-23T16:22:57.854603: step 310, graph loss 87.5621\n",
      "2017-09-23T16:22:59.575571: step 311, label loss 0.0407899, p1 0.28125, p3 0.239583, p5 0.171875\n",
      "2017-09-23T16:22:59.657434: step 311, graph loss 86.0605\n",
      "2017-09-23T16:23:01.443029: step 312, label loss 0.0424099, p1 0.28125, p3 0.161458, p5 0.140625\n",
      "2017-09-23T16:23:01.527489: step 312, graph loss 50.9296\n",
      "2017-09-23T16:23:03.189694: step 313, label loss 0.0393676, p1 0.25, p3 0.213542, p5 0.153125\n",
      "2017-09-23T16:23:03.273758: step 313, graph loss 74.2802\n",
      "2017-09-23T16:23:04.895468: step 314, label loss 0.0452445, p1 0.296875, p3 0.192708, p5 0.159375\n",
      "2017-09-23T16:23:04.976125: step 314, graph loss 101.798\n",
      "2017-09-23T16:23:06.600900: step 315, label loss 0.0439094, p1 0.28125, p3 0.234375, p5 0.16875\n",
      "2017-09-23T16:23:06.688235: step 315, graph loss 39.4037\n",
      "2017-09-23T16:23:08.343852: step 316, label loss 0.0468216, p1 0.265625, p3 0.203125, p5 0.14375\n",
      "2017-09-23T16:23:08.429306: step 316, graph loss 71.2299\n",
      "2017-09-23T16:23:10.065547: step 317, label loss 0.0397222, p1 0.265625, p3 0.192708, p5 0.134375\n",
      "2017-09-23T16:23:10.147105: step 317, graph loss 53.1795\n",
      "2017-09-23T16:23:11.776819: step 318, label loss 0.0422269, p1 0.359375, p3 0.203125, p5 0.153125\n",
      "2017-09-23T16:23:11.858951: step 318, graph loss 73.4866\n",
      "2017-09-23T16:23:13.503942: step 319, label loss 0.0415757, p1 0.234375, p3 0.15625, p5 0.128125\n",
      "2017-09-23T16:23:13.583487: step 319, graph loss 67.4661\n",
      "2017-09-23T16:23:15.301319: step 320, label loss 0.0443878, p1 0.21875, p3 0.1875, p5 0.14375\n",
      "2017-09-23T16:23:15.382222: step 320, graph loss 79.4507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-23T16:23:17.120478: step 321, label loss 0.0416058, p1 0.234375, p3 0.1875, p5 0.175\n",
      "2017-09-23T16:23:17.201201: step 321, graph loss 69.5841\n",
      "2017-09-23T16:23:18.901790: step 322, label loss 0.0434087, p1 0.234375, p3 0.166667, p5 0.13125\n",
      "2017-09-23T16:23:18.980938: step 322, graph loss 84.3389\n",
      "2017-09-23T16:23:20.686048: step 323, label loss 0.0462446, p1 0.203125, p3 0.166667, p5 0.153125\n",
      "2017-09-23T16:23:20.768140: step 323, graph loss 61.0291\n",
      "2017-09-23T16:23:22.411857: step 324, label loss 0.0433876, p1 0.15625, p3 0.166667, p5 0.15\n",
      "2017-09-23T16:23:22.498293: step 324, graph loss 76.9003\n",
      "2017-09-23T16:23:24.204735: step 325, label loss 0.039667, p1 0.171875, p3 0.140625, p5 0.121875\n",
      "2017-09-23T16:23:24.289476: step 325, graph loss 60.3389\n",
      "2017-09-23T16:23:25.953371: step 326, label loss 0.0443136, p1 0.265625, p3 0.177083, p5 0.146875\n",
      "2017-09-23T16:23:26.037760: step 326, graph loss 74.1743\n",
      "2017-09-23T16:23:27.754932: step 327, label loss 0.0376907, p1 0.28125, p3 0.213542, p5 0.171875\n",
      "2017-09-23T16:23:27.839787: step 327, graph loss 82.5156\n",
      "2017-09-23T16:23:29.485630: step 328, label loss 0.0467507, p1 0.171875, p3 0.140625, p5 0.11875\n",
      "2017-09-23T16:23:29.572174: step 328, graph loss 72.3911\n",
      "2017-09-23T16:23:31.230278: step 329, label loss 0.0439566, p1 0.296875, p3 0.166667, p5 0.13125\n",
      "2017-09-23T16:23:31.321565: step 329, graph loss 73.1633\n",
      "2017-09-23T16:23:33.006507: step 330, label loss 0.0424405, p1 0.203125, p3 0.161458, p5 0.1375\n",
      "2017-09-23T16:23:33.087160: step 330, graph loss 90.0119\n",
      "2017-09-23T16:23:34.761826: step 331, label loss 0.0463693, p1 0.296875, p3 0.1875, p5 0.13125\n",
      "2017-09-23T16:23:34.839836: step 331, graph loss 75.0203\n",
      "2017-09-23T16:23:36.500606: step 332, label loss 0.045868, p1 0.234375, p3 0.151042, p5 0.140625\n",
      "2017-09-23T16:23:36.580054: step 332, graph loss 49.1792\n",
      "2017-09-23T16:23:38.250534: step 333, label loss 0.0382373, p1 0.21875, p3 0.203125, p5 0.153125\n",
      "2017-09-23T16:23:38.333100: step 333, graph loss 78.9535\n",
      "2017-09-23T16:23:40.011472: step 334, label loss 0.0452256, p1 0.265625, p3 0.197917, p5 0.153125\n",
      "2017-09-23T16:23:40.098821: step 334, graph loss 69.6813\n",
      "2017-09-23T16:23:41.758991: step 335, label loss 0.0415631, p1 0.265625, p3 0.166667, p5 0.128125\n",
      "2017-09-23T16:23:41.839452: step 335, graph loss 57.5993\n",
      "2017-09-23T16:23:43.465215: step 336, label loss 0.0406932, p1 0.28125, p3 0.171875, p5 0.159375\n",
      "2017-09-23T16:23:43.548098: step 336, graph loss 103.314\n",
      "2017-09-23T16:23:45.174650: step 337, label loss 0.0436896, p1 0.234375, p3 0.177083, p5 0.134375\n",
      "2017-09-23T16:23:45.257131: step 337, graph loss 55.8476\n",
      "2017-09-23T16:23:46.893831: step 338, label loss 0.0388879, p1 0.1875, p3 0.140625, p5 0.125\n",
      "2017-09-23T16:23:46.975972: step 338, graph loss 65.4692\n",
      "2017-09-23T16:23:48.571751: step 339, label loss 0.0424412, p1 0.25, p3 0.203125, p5 0.165625\n",
      "2017-09-23T16:23:48.660079: step 339, graph loss 74.1223\n",
      "2017-09-23T16:23:50.313897: step 340, label loss 0.0397292, p1 0.25, p3 0.166667, p5 0.140625\n",
      "2017-09-23T16:23:50.400322: step 340, graph loss 79.378\n",
      "2017-09-23T16:23:52.053681: step 341, label loss 0.0406953, p1 0.25, p3 0.161458, p5 0.153125\n",
      "2017-09-23T16:23:52.137812: step 341, graph loss 54.7803\n",
      "2017-09-23T16:23:53.751128: step 342, label loss 0.043816, p1 0.203125, p3 0.140625, p5 0.121875\n",
      "2017-09-23T16:23:53.839798: step 342, graph loss 56.97\n",
      "2017-09-23T16:23:55.515138: step 343, label loss 0.0401842, p1 0.265625, p3 0.192708, p5 0.15625\n",
      "2017-09-23T16:23:55.599439: step 343, graph loss 87.7309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-530ffab34bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# train label part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_node_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mtrain_label_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_node_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# one step for label training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-530ffab34bc1>\u001b[0m in \u001b[0;36mtrain_label_step\u001b[0;34m(x_batch, y_batch_binary, y_batch_labels, node_ids, writer)\u001b[0m\n\u001b[1;32m     95\u001b[0m             _, step, summaries, label_loss, p1, p3, p5 = sess.run(\n\u001b[1;32m     96\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mlabel_train_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 feed_dict)\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             print(\"{}: step {}, label loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        with tf.name_scope('kim_cnn'):\n",
    "            cnn = KimCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=num_classes,\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                loss_function=FLAGS.loss_function,\n",
    "                redefine_output_layer=True)\n",
    "\n",
    "        with tf.name_scope('dw'):\n",
    "            dw = Word2Vec(FLAGS.dw_num_negative_samples,\n",
    "                          vocabulary_size,\n",
    "                          FLAGS.dw_embedding_size)\n",
    "        \n",
    "        with tf.name_scope('combined'):\n",
    "            model = Combined(cnn, dw)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        label_train_op = tf.train.AdamOptimizer(1e-3).minimize(model.label_loss)        \n",
    "        graph_train_op = tf.train.GradientDescentOptimizer(1.0).minimize(model.graph_loss)\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", 'combined'))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        if tf.gfile.Exists(out_dir):\n",
    "            print('cleaning ', out_dir)\n",
    "            tf.gfile.DeleteRecursively(out_dir)\n",
    "        tf.gfile.MakeDirs(out_dir)\n",
    "        \n",
    "        # Summaries for loss and precision\n",
    "        label_loss_summary = tf.summary.scalar(\"label_loss\", model.label_loss)\n",
    "        graph_loss_summary = tf.summary.scalar(\"graph_loss\", model.graph_loss)        \n",
    "        p1 = tf.summary.scalar(\"p1\", model.p1)\n",
    "        p3 = tf.summary.scalar(\"p3\", model.p3)\n",
    "        p5 = tf.summary.scalar(\"p5\", model.p5)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([label_loss_summary, graph_loss_summary,\n",
    "                                             p1, p3, p5])\n",
    "        \n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([label_loss_summary, graph_loss_summary,\n",
    "                                           p1, p3, p5])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())        \n",
    "        \n",
    "        def train_label_step(x_batch, y_batch_binary, y_batch_labels, node_ids, writer):\n",
    "            \"\"\"\n",
    "            one training step for the label part\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              model.cnn.input_x: x_batch,\n",
    "              model.cnn.input_y_binary: y_batch_binary,\n",
    "              model.cnn.input_y_labels: label_lists_to_sparse_tuple(\n",
    "                  y_batch_labels, num_classes),  # needs some conversion\n",
    "              model.node_ids: node_ids, # node ids\n",
    "              model.cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "                \n",
    "              # the following is in vain\n",
    "              # tf requires all placeholder to be provided some value                \n",
    "              model.dw.train_inputs: [0],\n",
    "              model.dw.train_labels: [[0]],\n",
    "            }\n",
    "            _, step, summaries, label_loss, p1, p3, p5 = sess.run(\n",
    "                [label_train_op, global_step, train_summary_op, model.label_loss, model.p1, model.p3, model.p5],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, label loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
    "                time_str, step, label_loss, p1, p3, p5))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def train_graph_step(x_batch, batch_labels, writer):\n",
    "            \"\"\"\n",
    "            one training step for the graph part\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              model.dw.train_inputs: x_batch,\n",
    "              model.dw.train_labels: np.expand_dims(np.array(batch_labels), -1),\n",
    "                \n",
    "              # the following is in vain\n",
    "              # tf requires all placeholder to be provided some value\n",
    "              model.cnn.input_x: list(vocab_processor.transform([\"asdfkjahdkfhakslfh\"])),  # non-sense stuff\n",
    "              model.cnn.input_y_binary: [[0] * num_classes],  # with no label\n",
    "              model.cnn.input_y_labels: label_lists_to_sparse_tuple(\n",
    "                  [[0]], num_classes),  # needs some conversion\n",
    "              model.node_ids: [0], # node ids\n",
    "              model.cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "                \n",
    "            }\n",
    "            _, step, summaries, graph_loss = sess.run(\n",
    "                [graph_train_op, global_step, train_summary_op, model.graph_loss],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, graph loss {:g}\".format(\n",
    "                time_str, step, graph_loss))\n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "            \n",
    "        def dev_step(x_batch, y_batch_binary, y_batch_labels, node_ids, writer):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              model.cnn.input_x: x_batch,\n",
    "              model.cnn.input_y_binary: y_batch_binary,\n",
    "              model.cnn.input_y_labels: label_lists_to_sparse_tuple(\n",
    "                  y_batch_labels, num_classes),  # needs some conversion\n",
    "              model.node_ids: node_ids, # node ids                \n",
    "              model.cnn.dropout_keep_prob: 1.0,\n",
    "                \n",
    "              # in vain\n",
    "              model.dw.train_inputs: [0],\n",
    "              model.dw.train_labels: [[0]],                \n",
    "            }\n",
    "            step, summaries, label_loss, p1, p3, p5 = sess.run(\n",
    "                [global_step, dev_summary_op, model.label_loss, model.p1, model.p3, model.p5],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, label loss {:g}, p1 {:g}, p3 {:g}, p5 {:g}\".format(\n",
    "                time_str, step, label_loss, p1, p3, p5))\n",
    "            \n",
    "            writer.add_summary(summaries, step)\n",
    "\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train_binary, y_train_labels, train_node_ids)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "        for batch in batches:\n",
    "            # train label part\n",
    "            x_batch, y_batch_binary, y_train_labels, x_node_ids = zip(*batch)\n",
    "            train_label_step(x_batch, y_batch_binary, y_train_labels, x_node_ids, train_summary_writer)\n",
    "            current_step = tf.train.global_step(sess, global_step)  # one step for label training\n",
    "            \n",
    "            # train graph part\n",
    "            batch_inputs, batch_labels = dw_data_generator.next_batch()\n",
    "            train_graph_step(batch_inputs, batch_labels, train_summary_writer)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)  # one step for graph training\n",
    "            \n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev_binary, y_dev_labels, dev_node_ids, dev_summary_writer)\n",
    "                print(\"\")\n",
    "                \n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "            \n",
    "            global_step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
